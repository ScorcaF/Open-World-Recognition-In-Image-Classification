{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"iCaRL_extension.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DTVuAYnFkXtK"},"source":["# Final extension classifier"]},{"cell_type":"code","metadata":{"id":"ZRtGl3OH9GbH"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n","\n","from math import floor\n","\n","from torchvision import transforms\n","from torchvision.models import resnet34\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","import numpy as np\n","import numpy.ma as ma\n","\n","from PIL import Image\n","from copy import deepcopy\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from torch.backends import cudnn\n","from torch.autograd import Variable\n","\n","import numpy as np\n","import numpy.ma as ma\n","from math import floor\n","from copy import deepcopy\n","import random\n","\n","sigmoid = nn.Sigmoid() # Sigmoid function\n","\n","\n","\n","class Exemplars(torch.utils.data.Dataset):\n","    def __init__(self, exemplars, transform=None):\n","        \n","        self.dataset = []\n","        self.targets = []\n","\n","        for y, exemplar_y in enumerate(exemplars):\n","            self.dataset.extend(exemplar_y)  \n","            self.targets.extend([y] * len(exemplar_y))  # return: [y,y,y,y, ... ] until len(exemplar_y)\n","\n","        self.transform = transform\n","    \n","    def __getitem__(self, index):\n","        image = self.dataset[index]\n","        target = self.targets[index]\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","class iCaRL_ext:\n","    \n","    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform):\n","        self.device = device\n","        self.net = net\n","\n","        # Set hyper-parameters\n","        self.LR = lr\n","        self.MOMENTUM = momentum\n","        self.WEIGHT_DECAY = weight_decay\n","        self.MILESTONES = milestones\n","        self.GAMMA = gamma\n","        self.NUM_EPOCHS = num_epochs\n","        self.BATCH_SIZE = batch_size\n","        \n","        # Set transformations\n","        self.train_transform = train_transform\n","        self.test_transform = test_transform\n","\n","        # List of exemplar sets: Each set contains memory_size/(2*num_classes) exemplars\n","        # with num_classes the number of classes seen until now by the network.\n","        # we have one exemplar set for the class mean computation and another one\n","        # for the mean distance computation\n","        self.exemplars_mean = []\n","        self.exemplars_distance = []\n","\n","        # Initialize the copy of the old network, used to compute outputs of the\n","        # previous network for the distillation loss, to None. This is useful to\n","        # correctly apply the first function when training the network for the\n","        # first time.\n","        self.old_net = None\n","\n","        # Maximum number of exemplars\n","        self.memory_size = 2000\n","    \n","        # Loss function\n","        self.criterion = nn.BCEWithLogitsLoss()\n","\n","        # If True, test on the best model found (e.g., minimize loss). If False,\n","        # test on the last model build (of the last epoch).\n","        self.VALIDATE = False\n","\n","    def classify(self, batch, train_dataset=None):\n","\n","        batch_features = self.extract_features(batch) # (batch size, 64) \n","        for i in range(batch_features.size(0)):       \n","            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n","        batch_features = batch_features.to(self.device)\n","        \n","       \n","        if self.cached_means is None:\n","            print(\"Computing mean of exemplars and class radius... \", end=\"\")\n","\n","            self.cached_means = []\n","           \n","            self.cached_radius = []\n","           \n","\n","\n","            # Number of known classes\n","            num_classes = len(self.exemplars_mean)\n","\n","            # Compute the means of classes with all the data available,\n","            # including training data which contains samples belonging to\n","            # the latest 10 classes. This will remove noise from the mean\n","            # estimate, improving the results.  \n","            if train_dataset is not None:\n","                train_features_list = [[] for _ in range(10)]\n","\n","\n","                for train_sample, label in train_dataset: \n","                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform)\n","                    features = features/features.norm()\n","                    train_features_list[label % 10].append(features)\n","\n","\n","            for y in range(num_classes):\n","                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)): \n","                    features_list = train_features_list[y % 10]\n","                else:\n","                    features_list = []\n","                \n","           \n","                for exemplar in self.exemplars_mean[y]: \n","                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n","                    features = features/features.norm() # Normalize the feature representation of the exemplar\n","                    features_list.append(features)\n","                \n","                features_list = torch.stack(features_list)\n","                class_means = features_list.mean(dim=0)\n","                class_means = class_means/class_means.norm() # Normalize the class means\n","\n","                self.cached_means.append(class_means)\n","\n","\n","\n","                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)): \n","                    features_list2 = train_features_list[y % 10]\n","                else:\n","                    features_list2 = []\n","                \n","              \n","                for exemplar_distance in self.exemplars_distance[y]: \n","                    features_distance = self.extract_features(exemplar_distance, batch=False, transform=self.test_transform)\n","                    features_distance = features_distance/features_distance.norm() # Normalize the feature representation of the exemplar\n","                    features_list2.append(features_distance)\n","                \n","                features_list2 = torch.stack(features_list2)\n","\n","                class_radiuses = torch.norm(features_list2 - class_means, dim=1)    #we compute the mean class radius\n","                class_mean_radius = torch.mean(class_radiuses)\n","\n","                self.cached_radius.append(class_mean_radius)\n","\n","\n","            self.cached_radius = torch.stack(self.cached_radius).to(self.device) \n","            self.cached_means = torch.stack(self.cached_means).to(self.device)\n","            print(\"done\")\n","        \n","        #Classification\n","        preds = []\n","        for i in range(batch_features.size(0)): \n","            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1)\n","            new_f_arg = f_arg / self.cached_radius\n","            preds.append(torch.argmin(new_f_arg)) \n","        return torch.stack(preds)\n","\n","\n","\n","    \n","    def extract_features(self, sample, batch=True, transform=None):\n","\n","        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\" \n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        if batch is False: # Treat sample as single PIL image \n","            sample = transform(sample)\n","            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336, (3, 32, 32) --> (1, 3, 32, 32)\n","\n","        sample = sample.to(self.device)\n","       \n","        \n","\n","        if self.VALIDATE:\n","            features = self.best_net.features(sample)\n","        else:\n","            features = self.net.features(sample)   \n","\n","        if batch is False:\n","            features = features[0] \n","\n","        return features\n","\n","    def incremental_train(self, split, train_dataset, val_dataset):\n","\n","        if split is not 0:\n","            # Increment the number of output nodes for the new network by 10\n","            #starting from 1 (at run 0 we already have 10 output nodes)\n","            self.increment_classes(10)\n","\n","        # Improve network parameters upon receiving new classes. Effectively\n","        # train a new network starting from the current network parameters.\n","\n","       \n","        train_logs = self.update_representation(train_dataset, val_dataset) \n","        \n","       \n","        num_classes = self.output_neurons_count()\n","        m = floor(self.memory_size / (2*num_classes))     # we have 2 exemplar set, so each one will have half size of exemplars\n","\n","        print(f\"Target number of exemplars per class: {m}\")\n","        print(f\"Target total number of exemplars for the mean: {m*num_classes}\")\n","        print(f\"Target total number of exemplars for the distance: {m*num_classes}\")\n","\n","        # Reduce pre-existing exemplar sets in order to fit new exemplars:\n"," \n","        for y in range(len(self.exemplars_mean)):\n","            self.exemplars_mean[y] = self.reduce_exemplar_set(self.exemplars_mean[y], m)\n","\n","        for y in range(len(self.exemplars_distance)):\n","            self.exemplars_distance[y] = self.reduce_exemplar_set(self.exemplars_distance[y], m)\n","\n","        # Construct exemplar set for new classes: \n","      \n","        #exemplar set per class mean using herding\n","        new_exemplars = self.construct_exemplar_set_herding(train_dataset, m) \n","        self.exemplars_mean.extend(new_exemplars)\n","\n","        #exemplar set for the distances\n","        new_exemplars = self.construct_distance_exemplar_set(train_dataset, m) \n","        self.exemplars_distance.extend(new_exemplars)\n","\n","        return train_logs\n","\n","\n","\n","    def update_representation(self, train_dataset, val_dataset):\n","\n","        print(f\"Length of herding exemplars set: {sum([len(self.exemplars_mean[y]) for y in range(len(self.exemplars_mean))])}\")\n","        print(f\"Length of mean distance exemplars set: {sum([len(self.exemplars_distance[y]) for y in range(len(self.exemplars_distance))])}\")\n","        \n","        exemplars_mean_dataset = Exemplars(self.exemplars_mean, self.train_transform)\n","        exemplars_distance_dataset = Exemplars(self.exemplars_distance, self.train_transform)\n","\n","        # we train just with herding exemplar \n","        train_dataset_with_exemplars = ConcatDataset([exemplars_mean_dataset, exemplars_distance_dataset , train_dataset])\n","        \n","        # Train the network on combined dataset\n","        train_logs = self.train(train_dataset_with_exemplars, val_dataset) \n","\n","        # Keep a copy of the current network in order to compute its outputs for\n","        # the distillation loss while the new network is being trained.\n","        self.old_net = deepcopy(self.net)\n","\n","        return train_logs\n","\n","   \n","\n","   \n","\n","    def construct_exemplar_set_herding(self, dataset, m): \n","\n","        dataset.dataset.disable_transform()\n","\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range\n","            samples[label].append(image)\n","\n","        dataset.dataset.enable_transform()\n","\n","        # Initialize exemplar sets\n","        exemplars = [[] for _ in range(10)]\n","\n","        # Iterate over classes\n","        for y in range(10):\n","            print(f\"Extracting exemplars from class {y} of current split... \", end=\"\")\n","\n","            # Transform samples to tensors and apply normalization\n","            transformed_samples = torch.zeros((len(samples[y]), 3, 32, 32)).to(self.device)\n","            for i in range(len(transformed_samples)): \n","                transformed_samples[i] = self.test_transform(samples[y][i])\n","\n","            # Extract features from samples\n","            samples_features = self.extract_features(transformed_samples).to(self.device)\n","\n","            # Compute the feature mean of the current class\n","            features_mean = samples_features.mean(dim=0)\n","\n","            # Initializes indices vector, containing the index of each exemplar chosen\n","            idx = []\n","\n","            # See iCaRL algorithm 4\n","            for k in range(1, m+1): # k = 1, ..., m -- Choose m exemplars\n","                if k == 1: # No exemplars chosen yet, sum to 0 vector\n","                    f_sum = torch.zeros(64).to(self.device)\n","                else: # Sum of features of all exemplars chosen until now (j = 1, ..., k-1)\n","                    f_sum = samples_features[idx].sum(dim=0)\n","\n","                # Compute argument of argmin function\n","                f_arg = torch.norm(features_mean - 1/k * (samples_features + f_sum), dim=1) \n","                \n","\n","                # Mask exemplars that were already taken, as we do not want to store the\n","                # same exemplar more than once\n","                mask = np.zeros(len(f_arg), int)\n","                mask[idx] = 1\n","                f_arg_masked = ma.masked_array(f_arg.cpu().detach().numpy(), mask=mask) \n","                \n","                # Compute the nearest available exemplar\n","                exemplar_idx = np.argmin(f_arg_masked)\n","\n","                idx.append(exemplar_idx)\n","            \n","            # Save exemplars to exemplar set\n","            for i in idx:\n","                exemplars[y].append(samples[y][i])\n","            \n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","            \n","        return exemplars\n","\n","\n","    def construct_distance_exemplar_set(self, dataset, m):\n","\n","        dataset.dataset.disable_transform()\n","\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range\n","            samples[label].append(image)\n","\n","        dataset.dataset.enable_transform()\n","\n","        # Initialize exemplar sets\n","        exemplars = [[] for _ in range(10)]\n","\n","        # Iterate over classes\n","        for y in range(10):\n","            print(f\"Extracting exemplars from class {y} of current split... \", end=\"\")\n","\n","            # Transform samples to tensors and apply normalization\n","            transformed_samples = torch.zeros((len(samples[y]), 3, 32, 32)).to(self.device)\n","            for i in range(len(transformed_samples)):  \n","                transformed_samples[i] = self.test_transform(samples[y][i])\n","\n","            # Extract features from samples\n","            samples_features = self.extract_features(transformed_samples).to(self.device)\n","\n","            # Compute the feature mean of the current class\n","            features_mean = samples_features.mean(dim=0)\n","\n","            #we need here to compute the mean distance:\n","            distances_from_mean = torch.norm(features_mean - samples_features, dim=1)\n","            distance_mean = distances_from_mean.mean() \n","\n","\n","            # Initializes indices vector, containing the index of each exemplar chosen\n","            idx = []\n","\n","            # See iCaRL algorithm 4\n","            for k in range(1, m+1): \n","                # Compute argument of argmin function\n","                f_arg = torch.abs(distance_mean - torch.norm(features_mean - samples_features, dim=1)) \n","\n","                # Mask exemplars that were already taken, as we do not want to store the\n","                # same exemplar more than once\n","                mask = np.zeros(len(f_arg), int)\n","                mask[idx] = 1\n","                f_arg_masked = ma.masked_array(f_arg.cpu().detach().numpy(), mask=mask)\n","\n","                # Compute the nearest available exemplar\n","                exemplar_idx = np.argmin(f_arg_masked)\n","\n","                idx.append(exemplar_idx)\n","            \n","            # Save exemplars to exemplar set\n","            for i in idx:\n","                exemplars[y].append(samples[y][i])\n","            \n","            print(f\"Extracted for distance exemplar set {len(exemplars[y])} exemplars.\")\n","            \n","        return exemplars\n","\n","\n","    def reduce_exemplar_set(self, exemplar_set, m):\n","\n","        return exemplar_set[:m]\n","    \n","    #\n","    #train is the same of standard train routine\n","    #\n","    def train(self, train_dataset, val_dataset):\n","        # Define the optimization algorithm\n","        parameters_to_optimize = self.net.parameters()\n","        self.optimizer = optim.SGD(parameters_to_optimize, \n","                                   lr=self.LR,\n","                                   momentum=self.MOMENTUM,\n","                                   weight_decay=self.WEIGHT_DECAY)\n","        \n","        # Define the learning rate decaying policy\n","        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n","                                                        milestones=self.MILESTONES,\n","                                                        gamma=self.GAMMA)\n","\n","        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","\n","        # Send networks to chosen device\n","        self.net = self.net.to(self.device)\n","        if self.old_net is not None: self.old_net = self.old_net.to(self.device)\n","\n","        cudnn.benchmark  # Calling this optimizes runtime\n","\n","        self.best_val_loss = float('inf')\n","        self.best_val_accuracy = 0\n","        self.best_train_loss = float('inf')\n","        self.best_train_accuracy = 0\n","        \n","        self.best_net = None\n","        self.best_epoch = -1\n","\n","        for epoch in range(self.NUM_EPOCHS):\n","            # Run an epoch (start counting form 1)\n","            train_loss, train_accuracy = self.do_epoch(epoch+1)\n","        \n","            # Validate after each epoch \n","            val_loss, val_accuracy = self.validate()    \n","\n","            # Validation criterion: best net is the one that minimizes the loss\n","            # on the validation set.\n","            if self.VALIDATE and val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.best_val_accuracy = val_accuracy\n","                self.best_train_loss = train_loss\n","                self.best_train_accuracy = train_accuracy\n","\n","                self.best_net = deepcopy(self.net)\n","                self.best_epoch = epoch\n","                print(\"Best model updated\")\n","\n","        if self.VALIDATE:\n","            val_loss = self.best_val_loss\n","            val_accuracy = self.best_val_accuracy\n","            train_loss = self.best_train_loss\n","            train_accuracy = self.best_train_accuracy\n","\n","            print(f\"Best model found at epoch {self.best_epoch+1}\")\n","\n","        return train_loss, train_accuracy, val_loss, val_accuracy\n","    \n","    def do_epoch(self, current_epoch):\n","\n","        # Set the current network in training mode\n","        self.net.train()\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_train_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n","\n","        for images, labels in self.train_dataloader:\n","            loss, corrects = self.do_batch(images, labels)\n","\n","            running_train_loss += loss.item()\n","            running_corrects += corrects\n","            total += labels.size(0)\n","            batch_idx += 1\n","\n","        self.scheduler.step()\n","\n","        # Calculate average scores\n","        train_loss = running_train_loss / batch_idx # Average over all batches\n","        train_accuracy = running_corrects / float(total) # Average over all samples\n","\n","        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n","\n","        return train_loss, train_accuracy\n","\n","    def do_batch(self, batch, labels):\n","\n","        batch = batch.to(self.device)\n","        labels = labels.to(self.device)\n","\n","        # Zero-ing the gradients\n","        self.optimizer.zero_grad()\n","        \n","       \n","\n","        num_classes = self.output_neurons_count() # Number of classes seen until now, including new classes\n","        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n","\n","        if self.old_net is None:\n","            # Network is training for the first time, so we only apply the\n","            # classification loss.\n","            targets = one_hot_labels\n","\n","        else:\n","           \n","            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10] \n","            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n","\n","        # Forward pass\n","        outputs = self.net(batch)\n","        loss = self.criterion(outputs, targets)\n","\n","        # Get predictions\n","        _, preds = torch.max(outputs.data, 1)\n","\n","        # Accuracy over NEW IMAGES, not over all images\n","        running_corrects = torch.sum(preds == labels.data).data.item() \n","\n","        # Backward pass: computes gradients\n","        loss.backward()\n","\n","        self.optimizer.step()\n","\n","        return loss, running_corrects\n","\n","    def validate(self):\n","\n","        self.net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_val_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        for images, labels in self.val_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","            total += labels.size(0)\n","\n","            # One hot encoding of new task labels \n","            one_hot_labels = self.to_onehot(labels)\n","\n","            # New net forward pass\n","            outputs = self.net(images)  \n","            loss = self.criterion(outputs, one_hot_labels) # loss type: BCE Loss with sigmoids over outputs\n","\n","            running_val_loss += loss.item()\n","\n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            # Update the number of correctly classified validation samples\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            batch_idx += 1\n","\n","        # Calculate scores\n","        val_loss = running_val_loss / batch_idx\n","        val_accuracy = running_corrects / float(total)\n","\n","        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n","\n","        return val_loss, val_accuracy\n","\n","    def test(self, test_dataset, train_dataset=None):\n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","        running_corrects = 0\n","        total = 0\n","\n","        # To store all predictions\n","        all_preds = torch.tensor([])\n","        all_preds = all_preds.type(torch.LongTensor)\n","        all_targets = torch.tensor([])\n","        all_targets = all_targets.type(torch.LongTensor)\n","\n","        # Clear mean of exemplars cache\n","        self.cached_means = None \n","        \n","        # Disable transformations for train_dataset, if available, as we will\n","        # need original PIL images from which to extract features.\n","        if train_dataset is not None: train_dataset.dataset.disable_transform()\n","\n","        for images, labels in self.test_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","\n","            total += labels.size(0)\n","            \n","            with torch.no_grad():\n","                preds = self.classify(images, train_dataset)\n","\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            all_targets = torch.cat(\n","                (all_targets.to(self.device), labels.to(self.device)), dim=0\n","            )\n","\n","            all_preds = torch.cat(\n","                (all_preds.to(self.device), preds.to(self.device)), dim=0\n","            )\n","\n","        if train_dataset is not None: train_dataset.dataset.enable_transform()\n","\n","        # Calculate accuracy\n","        accuracy = running_corrects / float(total)  \n","\n","        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n","\n","        if train_dataset is None:\n","            print(\"(only exemplars)\")\n","        else:\n","            print(\"(exemplars and training data)\")\n","\n","        return accuracy, all_targets, all_preds\n","\n","    \n","    \n","    def increment_classes(self, n=10):\n","\n","        in_features = self.net.fc.in_features  # size of each input sample\n","        out_features = self.net.fc.out_features  # size of each output sample\n","        weight = self.net.fc.weight.data\n","        bias = self.net.fc.bias.data\n","\n","        self.net.fc = nn.Linear(in_features, out_features+n)\n","        self.net.fc.weight.data[:out_features] = weight\n","        self.net.fc.bias.data[:out_features] = bias\n","    \n","    def output_neurons_count(self):\n","\n","        return self.net.fc.out_features\n","    \n","    def feature_neurons_count(self):\n","\n","        return self.net.fc.in_features\n","    \n","    def to_onehot(self, targets):\n","      \n","        num_classes = self.net.fc.out_features\n","        one_hot_targets = torch.eye(num_classes)[targets]\n","\n","        return one_hot_targets.to(self.device)\n","\n","    def network_params(self):\n","        weight = self.net.fc.weight.data\n","        bias = self.net.fc.bias.data\n","\n","        return weight, bias"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vti4hfPCkd7l"},"source":["# Semantic drift compensation"]},{"cell_type":"code","metadata":{"id":"ftxkvF996el0"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n","\n","from math import floor\n","\n","from torchvision import transforms\n","from torchvision.models import resnet34\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","import numpy as np\n","import numpy.ma as ma\n","\n","from PIL import Image\n","from copy import deepcopy\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from torch.backends import cudnn\n","from torch.autograd import Variable\n","\n","import numpy as np\n","import numpy.ma as ma\n","from math import floor\n","from copy import deepcopy\n","import random\n","\n","sigmoid = nn.Sigmoid() \n","\n","class Exemplars(torch.utils.data.Dataset):\n","    def __init__(self, exemplars, transform=None):\n","\n","\n","        self.dataset = []\n","        self.targets = []\n","\n","        for y, exemplar_y in enumerate(exemplars):\n","            self.dataset.extend(exemplar_y)\n","            self.targets.extend([y] * len(exemplar_y))  \n","\n","        self.transform = transform\n","    \n","    def __getitem__(self, index):\n","        image = self.dataset[index]\n","        target = self.targets[index]\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","class iCaRL_SDC:\n","    \n","\n","    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform):\n","        self.device = device\n","        self.net = net\n","        self.cached_means = None\n","\n","        # Set hyper-parameters\n","        self.LR = lr\n","        self.MOMENTUM = momentum\n","        self.WEIGHT_DECAY = weight_decay\n","        self.MILESTONES = milestones\n","        self.GAMMA = gamma\n","        self.NUM_EPOCHS = num_epochs\n","        self.BATCH_SIZE = batch_size\n","        \n","        # Set transformations\n","        self.train_transform = train_transform\n","        self.test_transform = test_transform\n","\n","\n","        self.exemplars = []\n","\n","        self.old_net = None\n","        self.previous_net = None\n","\n","        # Maximum number of exemplars\n","        self.memory_size = 2000\n","    \n","        # Loss function\n","        self.criterion = nn.BCEWithLogitsLoss()\n","\n"," \n","        self.VALIDATE = False\n","\n","\n","    def classify(self, batch, train_dataset=None):\n","        \n","\n","        batch_features = self.extract_features(batch) \n","        for i in range(batch_features.size(0)):      \n","            batch_features[i] = batch_features[i]/batch_features[i].norm()\n","        batch_features = batch_features.to(self.device)\n","        \n","        #Ottenimento prototipes\n","        if self.test_batch == 0: \n","            print(\"Computing mean of exemplars... \", end=\"\")\n","\n","            # Class means for the last ten classes\n","            self.new_cached_means = [] \n","            \n","            #Parameter used for Cifar 100 in \"Semantic Drift Compensation for Class-Incremental Learning\"\n","            sigma = 0.2 \n","\n","            # Number of known classes\n","            num_classes = len(self.exemplars)     \n","\n","\n","            if train_dataset is not None:\n","                train_features_list = [[] for _ in range(10)]\n","                drift_vectors = [] # list collecting difference in feature representations of new and old network\n","                all_weights = torch.tensor([]).to(self.device) \n","\n","\n","                # Computing prototypes\n","                if self.cached_means is None: \n","                  \n","                  for train_sample, label in train_dataset: \n","                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform) \n","                    features = features/features.norm()\n","                    train_features_list[label % 10].append(features)\n","    \n","                else:\n","                  \n","                  for train_sample, label in train_dataset:\n","                    #current net representation\n","                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform) \n","                    features = features/features.norm()\n","                    train_features_list[label % 10].append(features)\n","\n","                    #old net representation\n","                    old_features = self.extract_features(train_sample, batch=False, transform=self.test_transform, new = False)\n","                    old_features = old_features/features.norm()\n","                    \n","                    \n","                    sample_weights = (old_features - self.cached_means).unsqueeze(0) #size: (num cached means, 64)\n","                    all_weights = torch.cat((all_weights, sample_weights), dim = 0) \n","                    \n","                    # difference in the representation space for current sample\n","                    single_drift = features - old_features\n","\n","                    drift_vectors.append(single_drift)\n","\n","                  drift_vectors = torch.stack(drift_vectors) #size = (num samples, 64)\n","                  \n","\n","                  # Eq. 10 and 11 in \"Semantic Drift Compensation for Class-Incremental Learning\" (Par 4.1)\n","                  for j in range(len(self.cached_means)):\n","                   \n","                    allSamples_CurrentMean_difference = all_weights[:, j] #size: (num samples, 64); \n","                    \n","                    #Eq 11\n","                    class_weights = torch.exp(-1/(2*sigma**2)*torch.norm(allSamples_CurrentMean_difference, dim = 1)**2) #size = num samples\n","                    # Eq 10\n","                    semantic_drift = torch.sum(class_weights.reshape(-1, 1)*drift_vectors, dim = 0)/torch.sum(class_weights)\n","\n","                    # Updating position of old means, having estimated the shift in feature space in this split\n","                    self.cached_means[j] = self.cached_means[j] + semantic_drift\n","\n","            # Computing new means    \n","            for y in range(num_classes-10, num_classes):\n","                if (train_dataset is not None):\n","                    features_list = train_features_list[y % 10]\n","                    features_list = torch.stack(features_list)\n","\n","                    class_means = features_list.mean(dim=0) \n","                    class_means = class_means/class_means.norm() \n","                    self.new_cached_means.append(class_means)\n","\n","            \n","            self.new_cached_means = torch.stack(self.new_cached_means).to(self.device)\n","\n","            if self.cached_means is None: \n","              self.cached_means = self.new_cached_means\n","\n","            else:\n","              # Inserting new means\n","              self.cached_means = torch.cat((self.cached_means, self.new_cached_means), dim = 0)\n","            print(\"done\")\n","        \n","        #Classification\n","        preds = []\n","        for i in range(batch_features.size(0)): \n","            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1) \n","            preds.append(torch.argmin(f_arg)) \n","        \n","        return torch.stack(preds)\n","    \n","    def extract_features(self, sample, batch=True, transform=None, new = True):\n","\n","\n","        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\" \n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.previous_net is not None: self.previous_net.train(False)\n","\n","\n","\n","        if batch is False: \n","            sample = transform(sample)\n","            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336, (3, 32, 32) --> (1, 3, 32, 32)\n","\n","        sample = sample.to(self.device)\n","       \n","        \n","        if new: # compute feature representations of the current network\n","          if self.VALIDATE:\n","            features = self.best_net.features(sample)\n","          else:\n","            features = self.net.features(sample)\n","\n","        else: # compute feature representations of the previous network\n","          features = self.previous_net.features(sample)\n","\n","\n","        if batch is False:\n","            features = features[0] \n","        return features\n","\n","    def incremental_train(self, split, train_dataset, val_dataset):\n","\n","\n","        if split is not 0:\n","\n","            self.increment_classes(10)\n","\n","\n","        train_logs = self.update_representation(train_dataset, val_dataset) \n","        \n","        num_classes = self.output_neurons_count()\n","        m = floor(self.memory_size / num_classes)\n","\n","        print(f\"Target number of exemplars per class: {m}\")\n","        print(f\"Target total number of exemplars: {m*num_classes}\")\n","\n","        # Reduce pre-existing exemplar sets in order to fit new exemplars:\n","        for y in range(len(self.exemplars)):\n","            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n","\n","        # Construct exemplar set for new classes: \n","        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m) #dovremmo usare l'herding\n","        self.exemplars.extend(new_exemplars)\n","\n","        return train_logs\n","\n","    def update_representation(self, train_dataset, val_dataset):\n","\n","        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n","\n","        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n","        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n","\n","        # Train the network on combined dataset\n","        train_logs = self.train(train_dataset_with_exemplars, val_dataset) \n","        \n","        if self.old_net is not None: self.previous_net = deepcopy(self.old_net)\n","\n","        self.old_net = deepcopy(self.net)\n","        \n","\n","        return train_logs\n","\n","    def construct_exemplar_set_rand(self, dataset, m):\n","\n","        dataset.dataset.disable_transform()\n","        \n","        #storing images of a split\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range: example: 21 -> 1, 22 -> 2 etc.\n","            samples[label].append(image)\n","\n","\n","        dataset.dataset.enable_transform()\n","\n","        exemplars = [[] for _ in range(10)]     \n","        #random sampling\n","        for y in range(10):\n","            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n","            # Randomly choose m samples from samples[y] without replacement\n","            exemplars[y] = random.sample(samples[y], m)\n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","\n","        return exemplars\n","\n","\n","    def construct_exemplar_set_herding(self, dataset, m):\n","\n","        dataset.dataset.disable_transform()\n","\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range\n","            samples[label].append(image)\n","\n","        dataset.dataset.enable_transform()\n","\n","        # Initialize exemplar sets\n","        exemplars = [[] for _ in range(10)]\n","\n","        # Iterate over classes\n","        for y in range(10):\n","            print(f\"Extracting exemplars from class {y} of current split... \", end=\"\")\n","\n","            # Transform samples to tensors and apply normalization\n","            transformed_samples = torch.zeros((len(samples[y]), 3, 32, 32)).to(self.device)\n","            for i in range(len(transformed_samples)):  \n","                transformed_samples[i] = self.test_transform(samples[y][i])\n","\n","            # Extract features from samples\n","            samples_features = self.extract_features(transformed_samples).to(self.device)\n","\n","            # Compute the feature mean of the current class\n","            features_mean = samples_features.mean(dim=0)\n","\n","     \n","            idx = []\n","\n","            # See iCaRL algorithm 4\n","            for k in range(1, m+1): # k = 1, ..., m -- Choose m exemplars\n","                if k == 1: # No exemplars chosen yet, sum to 0 vector\n","                    f_sum = torch.zeros(64).to(self.device)\n","                else: # Sum of features of all exemplars chosen until now (j = 1, ..., k-1)\n","                    f_sum = samples_features[idx].sum(dim=0)\n","\n","                # Compute argument of argmin function\n","                f_arg = torch.norm(features_mean - 1/k * (samples_features + f_sum), dim=1)\n","                mask = np.zeros(len(f_arg), int)\n","                mask[idx] = 1\n","                f_arg_masked = ma.masked_array(f_arg.cpu().detach().numpy(), mask=mask) \n","\n","                # Compute the nearest available exemplar\n","                exemplar_idx = np.argmin(f_arg_masked)\n","\n","                idx.append(exemplar_idx)\n","            \n","            # Save exemplars to exemplar set\n","            for i in idx:\n","                exemplars[y].append(samples[y][i])\n","            \n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","            \n","        return exemplars\n","\n","    def reduce_exemplar_set(self, exemplar_set, m):\n","\n","        return exemplar_set[:m]\n","    \n","\n","    def train(self, train_dataset, val_dataset):\n","\n","\n","        # Define the optimization algorithm\n","        parameters_to_optimize = self.net.parameters()\n","        self.optimizer = optim.SGD(parameters_to_optimize, \n","                                   lr=self.LR,\n","                                   momentum=self.MOMENTUM,\n","                                   weight_decay=self.WEIGHT_DECAY)\n","        \n","        # Define the learning rate decaying policy\n","        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n","                                                        milestones=self.MILESTONES,\n","                                                        gamma=self.GAMMA)\n","\n","        # Create DataLoaders for training and validation\n","        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","\n","        # Send networks to chosen device\n","        self.net = self.net.to(self.device)\n","        if self.old_net is not None: self.old_net = self.old_net.to(self.device)\n","        if self.previous_net is not None: self.previous_net = self.previous_net.to(self.device)\n","\n","\n","        cudnn.benchmark  \n","\n","        self.best_val_loss = float('inf')\n","        self.best_val_accuracy = 0\n","        self.best_train_loss = float('inf')\n","        self.best_train_accuracy = 0\n","        \n","        self.best_net = None\n","        self.best_epoch = -1\n","\n","        for epoch in range(self.NUM_EPOCHS):\n","            # Run an epoch (start counting form 1)\n","            train_loss, train_accuracy = self.do_epoch(epoch+1)\n","        \n","            # Validate after each epoch \n","            val_loss, val_accuracy = self.validate()    \n","\n","\n","            if self.VALIDATE and val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.best_val_accuracy = val_accuracy\n","                self.best_train_loss = train_loss\n","                self.best_train_accuracy = train_accuracy\n","\n","                self.best_net = deepcopy(self.net)\n","                self.best_epoch = epoch\n","                print(\"Best model updated\")\n","\n","        if self.VALIDATE:\n","            val_loss = self.best_val_loss\n","            val_accuracy = self.best_val_accuracy\n","            train_loss = self.best_train_loss\n","            train_accuracy = self.best_train_accuracy\n","\n","            print(f\"Best model found at epoch {self.best_epoch+1}\")\n","\n","        return train_loss, train_accuracy, val_loss, val_accuracy\n","    \n","    def do_epoch(self, current_epoch):\n","\n","        # Set the current network in training mode\n","        self.net.train()\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","        if self.previous_net is not None: self.previous_net.train(False)\n","\n","        running_train_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n","\n","        for images, labels in self.train_dataloader:\n","            loss, corrects = self.do_batch(images, labels)\n","\n","            running_train_loss += loss.item()\n","            running_corrects += corrects\n","            total += labels.size(0)\n","            batch_idx += 1\n","\n","        self.scheduler.step()\n","\n","        # Calculate average scores\n","        train_loss = running_train_loss / batch_idx # Average over all batches\n","        train_accuracy = running_corrects / float(total) # Average over all samples\n","\n","        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n","\n","        return train_loss, train_accuracy\n","\n","    def do_batch(self, batch, labels):\n","\n","        batch = batch.to(self.device)\n","        labels = labels.to(self.device)\n","\n","        # Zero-ing the gradients\n","        self.optimizer.zero_grad()\n","        \n","        num_classes = self.output_neurons_count() \n","        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n","\n","        if self.old_net is None:\n","            # Network is training for the first time, so we only apply the\n","            # classification loss.\n","            targets = one_hot_labels\n","\n","        else:\n","            \n","            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10] \n","            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n","\n","        # Forward pass\n","        outputs = self.net(batch)\n","        loss = self.criterion(outputs, targets)\n","\n","        # Get predictions\n","        _, preds = torch.max(outputs.data, 1)\n","\n","       \n","        running_corrects = torch.sum(preds == labels.data).data.item() \n","\n","        # Backward pass: computes gradients\n","        loss.backward()\n","\n","        self.optimizer.step()\n","\n","        return loss, running_corrects\n","\n","    def validate(self): \n","\n","        self.net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_val_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        for images, labels in self.val_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","            total += labels.size(0)\n","\n","            # One hot encoding of new task labels \n","            one_hot_labels = self.to_onehot(labels)\n","\n","            # New net forward pass\n","            outputs = self.net(images)  \n","            loss = self.criterion(outputs, one_hot_labels) # loss type: BCE Loss with sigmoids over outputs\n","\n","            running_val_loss += loss.item()\n","\n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            # Update the number of correctly classified validation samples\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            batch_idx += 1\n","\n","        # Calculate scores\n","        val_loss = running_val_loss / batch_idx\n","        val_accuracy = running_corrects / float(total)\n","\n","        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n","\n","        return val_loss, val_accuracy\n","\n","    def test(self, test_dataset, train_dataset=None):\n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)  \n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.previous_net is not None: self.previous_net.train(False)\n","\n","\n","        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","        running_corrects = 0\n","        total = 0\n","\n","        # To store all predictions\n","        all_preds = torch.tensor([])\n","        all_preds = all_preds.type(torch.LongTensor)\n","        all_targets = torch.tensor([])\n","        all_targets = all_targets.type(torch.LongTensor)\n","\n","        # A new test begins, so we need to compute new class means and update the old ones\n","        self.test_batch = 0 \n","        \n","\n","        if train_dataset is not None: train_dataset.dataset.disable_transform()\n","\n","        for images, labels in self.test_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","\n","            total += labels.size(0)\n","            \n","            with torch.no_grad():\n","                preds = self.classify(images, train_dataset)\n","                \n","            self.test_batch += 1\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            all_targets = torch.cat(\n","                (all_targets.to(self.device), labels.to(self.device)), dim=0\n","            )\n","\n","            all_preds = torch.cat(\n","                (all_preds.to(self.device), preds.to(self.device)), dim=0\n","            )\n","\n","        if train_dataset is not None: train_dataset.dataset.enable_transform()\n","\n","        # Calculate accuracy\n","        accuracy = running_corrects / float(total)  \n","\n","        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n","\n","        if train_dataset is None:\n","            print(\"(only exemplars)\")\n","        else:\n","            print(\"(exemplars and training data)\")\n","\n","        return accuracy, all_targets, all_preds\n","\n","    \n","    def increment_classes(self, n=10):\n","\n","\n","        in_features = self.net.fc.in_features  # size of each input sample\n","        out_features = self.net.fc.out_features  # size of each output sample\n","        weight = self.net.fc.weight.data\n","        bias = self.net.fc.bias.data\n","\n","        self.net.fc = nn.Linear(in_features, out_features+n)\n","        self.net.fc.weight.data[:out_features] = weight\n","        self.net.fc.bias.data[:out_features] = bias\n","    \n","    def output_neurons_count(self):\n"," \n","\n","        return self.net.fc.out_features\n","    \n"," \n","    \n","    def to_onehot(self, targets):\n","\n","        num_classes = self.net.fc.out_features\n","        one_hot_targets = torch.eye(num_classes)[targets]\n","\n","        return one_hot_targets.to(self.device)\n","\n"],"execution_count":null,"outputs":[]}]}