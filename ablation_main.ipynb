{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ablation_main.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"akp--Dj9RBTj"},"source":["#import libraries and package\n","\n","\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"wkBUBd8bQ6HE"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n","\n","from torchvision import transforms\n","from torchvision.models import resnet34\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","from PIL import Image\n","from copy import deepcopy\n","\n","import numpy as np\n","import sys\n","import os\n","np.set_printoptions(threshold=sys.maxsize) #needed to print correctly the logs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_OSaUaoksjR"},"source":["righe di codice utili per importare da altri jupyter notebook delle funzioni. "]},{"cell_type":"code","metadata":{"id":"i2MXsnVKxbzx"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrWrAj7bjtv5"},"source":["%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto\"\n","!pip install import-ipynb\n","\n","import import_ipynb "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKzmzOrAkGiM"},"source":["#retrieve from folder 'data'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/data\"                          \n","from DataClean import Cifar100 as datasetManager   # from **nome jupyter notebook** import **classe o metodo da importare**\n","from Manager import Manager\n","\n","#retrieve from folder 'models'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/models\"\n","from resnet import resnet32\n","\n","\n","#retrieve from folder 'logs'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/logs\"\n","from Save_logs import save_logs\n","\n","#retrieve from folder 'ablation'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/ablation studies/ablation_models\"\n","from iCaRL_KNN import iCaRL_knn\n","from iCaRL_SVM import iCaRL_svm\n","from iCaRL_LessForgetCostraint import iCaRL_LFC\n","from iCaRL_loss import iCaRL_losses\n","from iCaRL_LWF import iCaRL_lwf\n","\n","\n","# return in the main project folder\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zVIwT_NfRboW"},"source":["# Arguments"]},{"cell_type":"code","metadata":{"id":"vl63oUufRbJN"},"source":["DEVICE=torch.device(\"cuda:0\")\n","DIR='/content/data'\n","\n","RANDOM_SEED = [1993,30,423]   \n","NUM_CLASSES = 100      \n","\n","# Training parameter: \n","BATCH_SIZE = 64         # Batch size \n","NUM_EPOCHS = 70         # Total number of training epochs\n","LR = 2                  # Initial learning rate\n","MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n","WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n","MILESTONES = [49, 63]   # Step down policy from iCaRL (MultiStepLR)\n","                        # Decrease the learning rate by gamma at each milestone\n","GAMMA = 0.2             # Gamma factor from iCaRL"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yD6XReTJSCX2"},"source":["train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),  \n","                                      transforms.RandomHorizontalFlip(),     \n","                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n","                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","\n","test_transform = transforms.Compose([transforms.ToTensor(),\n","                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SULeiW0AiKXw"},"source":["# Loss Studies"]},{"cell_type":"markdown","metadata":{"id":"GbXHRQVTHOXs"},"source":["## LWF loss\n","\n"]},{"cell_type":"code","metadata":{"id":"xZt_R0JzHO08"},"source":["RUN_NAME = 'iCarl_ablation_distillation_LWF'\n","\n","CUDA_LAUNCH_BLOCKING=1\n","NUM_RUNS = len(RANDOM_SEED)\n","\n","\n","#### customize hyperparameter for tuning ######\n","############ (debug purpose) ##################\n","LR = 0.1\n","NUM_EPOCHS = 70\n","WEIGHT_DECAY = 1e-4\n","###############################################\n","\n","\n","# Initialize logs\n","logs = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","   #download cifar100:\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","\n","    \n","  net = resnet32()\n","  icarl = iCaRL_lwf(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n","\n","  for split_i in range(10):\n","        print(f\"-- Split {split_i} of run {run_i} (SEED: {RANDOM_SEED[run_i]}) --\")\n","\n","        ############## 10-class split management: ##################\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","        ############## extraction of validation index ##############\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   #choose here the test size\n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)                        #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)                            #the original train_dataset\n","\n","\n","        ############## training + log generation ###################        \n","        icarl.incremental_train(split_i, train_data_split, val_data_split)\n","        \n","        logs[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","\n","save_logs(logs_icarl_herd,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pqIMew-8oAuA"},"source":["## L1 and L2 loss"]},{"cell_type":"code","metadata":{"id":"ttsMXogXn_fj"},"source":["CUDA_LAUNCH_BLOCKING=1\n","NUM_RUNS = len(RANDOM_SEED)\n","\n","#### customize hyperparameter for tuning ######\n","############ (debug purpose) ##################\n","LR = 0.3\n","NUM_EPOCHS = 70\n","WEIGHT_DECAY = 1e-4\n","###############################################\n","\n","\n","#set loss to test: CHOOSE HERE L1/L2 LOSS\n","cls_loss = nn.CrossEntropyLoss()\n","dist_loss = nn.MSELoss()              #nn.L1loss() here to test L1 loss\n","\n","# Initialize log file:\n","logs = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","  #download cifar100:\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","\n","    \n","  net = resnet32()\n","  icarl = iCaRL_losses(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, \n","                     train_transform, test_transform, cls_loss, dist_loss)\n","\n","  for split_i in range(10):\n","        print(f\"-- Split {split_i} of run {run_i} (SEED: {RANDOM_SEED[run_i]}) --\")\n","\n","        ############## 10-class split management: ##################\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","        ############## extraction of validation index ##############\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   #choose here the test size\n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)                        #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)                            #the original train_dataset\n","\n","        ############## training + log generation ###################\n","\n","        icarl.incremental_train(split_i, train_data_split, val_data_split)\n","      \n","        logs[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","save_logs(logs,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6SOoqm_EbAMp"},"source":["## LFC loss"]},{"cell_type":"code","metadata":{"id":"umTXpDihbCjT"},"source":["RUN_NAME = 'iCarl_ablation_distillation_LFC'\n","\n","CUDA_LAUNCH_BLOCKING=1\n","NUM_RUNS = len(RANDOM_SEED)\n","\n","torch.autograd.set_detect_anomaly(True)\n","\n","#### customize hyperparameter for tuning ######\n","############ (debug purpose) ##################\n","LR = 0.3\n","NUM_EPOCHS = 70\n","WEIGHT_DECAY = 1e-4\n","###############################################\n","\n","\n","# Initialize logs\n","logs = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","   #download cifar100:\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","\n","    \n","  net = resnet32()\n","  icarl = iCaRL_LFC(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n","\n","  for split_i in range(10):\n","        print(f\"-- Split {split_i} of run {run_i} (SEED: {RANDOM_SEED[run_i]}) --\")\n","\n","        ############## 10-class split management: ##################\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","        ############## extraction of validation index ##############\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   #choose here the test size\n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)                        #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)                            #the original train_dataset\n","\n","\n","        ############## training + log generation ###################        \n","        icarl.incremental_train(split_i, train_data_split, val_data_split)\n","\n","        #### if run gets interrupted, we can uncomment this and save the model at each split\n","        #torch.save(icarl, \"model_lfc.pt\")\n","        \n","        logs[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","\n","save_logs(logs_icarl_herd,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mS27GMzIiPh6"},"source":["# classifier Studies\n","class_params are loaded with the best perfomance set found. Use as training parameter the ones declared in 'arguments' section."]},{"cell_type":"markdown","metadata":{"id":"rveSKWUZHft9"},"source":["## Ablation studies: SVM as classifier\n","\n"]},{"cell_type":"code","metadata":{"id":"j7C-FMz2HioR"},"source":["RUN_NAME = 'iCarl_SVM_alldata'\n","\n","#### debug purpose ######\n","NUM_RUNS = len(RANDOM_SEED)\n","NUM_EPOCHS = 70\n","#########################\n","\n","###chose hyperparameters for ablation: ###\n","all_data = True\n","criterion = nn.BCEWithLogitsLoss()\n","class_params = {'kernel':'linear', 'tol':0.0001}\n","\n","# Initialize logs\n","logs = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","  #download cifar100:\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n"," \n","  net = resnet32()\n","  icarl = iCaRL_svm(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES,\n","                    GAMMA, NUM_EPOCHS, BATCH_SIZE, \n","                    train_transform, test_transform,  criterion, class_params, all_data)\n","\n","  for split_i in range(10):\n","        print(f\"-- Split {split_i} of run {run_i} (SEED: {RANDOM_SEED[run_i]}) --\")\n","\n","        ############## 10-class split management: ##################\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","        ############## extraction of validation index ##############\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   #choose here the test size\n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)                        #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)                            #the original train_dataset\n","\n","    \n","        ############## training + log generation ###################\n","        train_logs = icarl.incremental_train(split_i, train_data_split, val_data_split)\n","\n","       \n","        logs[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","save_logs(logs_icarl_herd,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4OvJBQ7H14z"},"source":["## Ablation studies: KNN as classifier"]},{"cell_type":"code","metadata":{"id":"i-Q0Bk4iH2fn"},"source":["RUN_NAME = 'iCarl_knn_all_data'\n","\n","#### debug purpose ######\n","NUM_RUNS = len(RANDOM_SEED)\n","NUM_EPOCHS = 70\n","#########################\n","\n","#set if we want to train classifier with all data or only exemplars\n","all_data = True\n","criterion = nn.BCEWithLogitsLoss()\n","class_params = {\n","                'n_neighbors': 10,\n","                'weights': 'distance'\n","                      }\n","\n","logs = [[] for _ in range((len(RANDOM_SEED)))]\n","\n","for run_i in range(len(RANDOM_SEED)):\n","    \n","    #download cifar100:\n","    train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","    test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","    \n","    net = resnet32()\n","    icarl_knn = iCaRL_knn(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES,\n","                          GAMMA, NUM_EPOCHS, BATCH_SIZE,\n","                          train_transform, test_transform,  criterion, class_params, all_data)\n","\n","    for split_i in range(10):\n","        print(f\"## Split {split_i} of run {run_i} (SEED: {RANDOM_SEED[run_i]}) ##\")\n","        \n","        ############## 10-class split management: ##################\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","        ############## extraction of validation index ##############\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      \n","                                              random_state=RANDOM_SEED[run_i],\n","                                              test_size = 0.1)  #choose here the test size\n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)       #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)           #the original train_dataset\n","\n","\n","        # training + log generation\n","        icarl_knn.incremental_train(split_i, train_data_split, val_data_split)\n","        \n","        logs[run_i].append({})\n","\n","        # test + log generation\n","        acc, all_targets, all_preds = icarl_knn.test(test_dataset, train_data_split)\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","\n","save_logs(logs,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wmMwTstOAIu"},"source":[""],"execution_count":null,"outputs":[]}]}