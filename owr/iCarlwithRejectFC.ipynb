{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"iCarlwithRejectFC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"EFJinxpCUYnz"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from torch.backends import cudnn\n","\n","import numpy as np\n","import numpy.ma as ma\n","from math import floor\n","from copy import deepcopy\n","import random\n","\n","softmax = nn.Softmax(dim=None)\n","\n","sigmoid = nn.Sigmoid() \n","\n","class Exemplars(torch.utils.data.Dataset):\n","    def __init__(self, exemplars, transform=None):\n","       \n","        self.dataset = []\n","        self.targets = []\n","\n","        for y, exemplar_y in enumerate(exemplars):\n","            self.dataset.extend(exemplar_y)  \n","            self.targets.extend([y] * len(exemplar_y)) \n","\n","        self.transform = transform\n","    \n","    def __getitem__(self, index):\n","        image = self.dataset[index]\n","        target = self.targets[index]\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","class iCaRLWithRejectFC:\n","\n","    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform):\n","        self.device = device\n","        self.net = net\n","\n","        # Set hyper-parameters\n","        self.LR = lr\n","        self.MOMENTUM = momentum\n","        self.WEIGHT_DECAY = weight_decay\n","        self.MILESTONES = milestones\n","        self.GAMMA = gamma\n","        self.NUM_EPOCHS = num_epochs\n","        self.BATCH_SIZE = batch_size\n","        \n","        # Set transformations\n","        self.train_transform = train_transform\n","        self.test_transform = test_transform\n","\n","        # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n","        # with num_classes the number of classes seen until now by the network.\n","        self.exemplars = []\n","\n","        # Initialize the copy of the old network, used to compute outputs of the\n","        # previous network for the distillation loss, to None. This is useful to\n","        # correctly apply the first function when training the network for the\n","        # first time.\n","        self.old_net = None\n","\n","        # Maximum number of exemplars\n","        self.memory_size = 2000\n","    \n","        # Loss function\n","        self.criterion = nn.BCEWithLogitsLoss()\n","\n","        # If True, test on the best model found (e.g., minimize loss). If False,\n","        # test on the last model build (of the last epoch).\n","        self.VALIDATE = False\n","        self.activations_var = []\n","\n","\n","    def test_without_classifier(self, test_dataset, threshold):\n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False) # Set Network to evaluation mode\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","        self.threshold = threshold\n","\n","        running_corrects = 0\n","        total = 0\n","        running_unknown = 0 \n","\n","\n","        all_preds = torch.tensor([]) \n","        all_preds = all_preds.type(torch.LongTensor)\n","        all_targets = torch.tensor([])\n","        all_targets = all_targets.type(torch.LongTensor)\n","        \n","        for images, labels in self.test_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","            total += labels.size(0)\n","\n","            # Forward Pass\n","            with torch.no_grad():\n","                if self.VALIDATE:\n","                    outputs = self.best_net(images)\n","                else:\n","                    outputs = self.net(images)\n","\n","            probabilities = softmax(outputs)\n","        \n","\n","            # Get predictions\n","            max_probabilities, preds = torch.max(probabilities.data, 1)\n","\n","\n","            # OWR: increment number of unknown images seen so far and label them as 101 (unknown)\n","            for i in range(len(preds)):\n","                if max_probabilities[i].data.item() < self.threshold:\n","                    running_unknown += 1\n","                    preds[i] = 101\n","\n","\n","            # Update Corrects\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            all_targets = torch.cat(\n","                (all_targets.to(self.device), labels.to(self.device)), dim=0\n","            )\n","\n","            # Append batch predictions\n","            all_preds = torch.cat(\n","                (all_preds.to(self.device), preds.to(self.device)), dim=0\n","            )\n","\n","\n","        # Calculate accuracy and unknown ratio\n","        accuracy = running_corrects / float(total) \n","        unknown_ratio = running_unknown / float(total) \n"," \n","\n","        print(f\"Test accuracy (hybrid1-FC): {accuracy}\")\n","        print(f\"Unknown Ratio: {unknown_ratio} \", end=\"\")\n","\n","\n","        return accuracy, unknown_ratio, all_targets, all_preds\n","    \n","    \n","    def extract_features(self, sample, batch=True, transform=None):\n","\n","        assert not (batch is False and transform is None)\n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        if batch is False: \n","            sample = transform(sample)\n","            sample = sample.unsqueeze(0) \n","\n","        sample = sample.to(self.device)\n","\n","        if self.VALIDATE: \n","            features = self.best_net.features(sample)\n","        else:\n","            features = self.net.features(sample)\n","\n","        if batch is False:\n","            features = features[0] \n","\n","        return features\n","\n","    def incremental_train(self, split, train_dataset, val_dataset):\n","\n","        if split is not 0:\n","            # Increment the number of output nodes for the new network by 10\n","            self.increment_classes(10)\n","\n","        # Improve network parameters upon receiving new classes. Effectively\n","        # train a new network starting from the current network parameters.\n","        train_logs = self.update_representation(train_dataset, val_dataset) \n","\n","        # Compute the number of exemplars per class\n","        num_classes = self.output_neurons_count()\n","        m = floor(self.memory_size / num_classes)\n","\n","        print(f\"Target number of exemplars per class: {m}\")\n","        print(f\"Target total number of exemplars: {m*num_classes}\")\n","\n","        # Reduce pre-existing exemplar sets in order to fit new exemplars\n","        for y in range(len(self.exemplars)):\n","            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n","\n","        # Construct exemplar set for new classes\n","        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m) \n","        self.exemplars.extend(new_exemplars) \n","        \n","\n","        return train_logs\n","\n","    def update_representation(self, train_dataset, val_dataset): \n","\n","        # Combine the new training data with existing exemplars.\n","\n","        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n","\n","        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n","        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n","\n","        # Train the network on combined dataset\n","        train_logs = self.train(train_dataset_with_exemplars, val_dataset) \n","\n","        # Keep a copy of the current network in order to compute its outputs for\n","        # the distillation loss while the new network is being trained.\n","        self.old_net = deepcopy(self.net)\n","\n","        return train_logs\n","\n","\n","    def construct_exemplar_set_rand(self, dataset, m):\n","\n","        dataset.dataset.disable_transform()\n","        \n","        #storing images of a split\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range\n","            samples[label].append(image)\n","\n","        dataset.dataset.enable_transform()\n","\n","        exemplars = [[] for _ in range(10)]\n","        \n","        #sampling\n","        for y in range(10):\n","            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n","\n","            # Randomly choose m samples from samples[y] without replacement\n","            exemplars[y] = random.sample(samples[y], m)\n","\n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","\n","        return exemplars\n","\n","    def construct_exemplar_set_herding(self, dataset, m): \n","\n","        dataset.dataset.disable_transform()\n","\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range\n","            samples[label].append(image)\n","\n","        dataset.dataset.enable_transform()\n","\n","        # Initialize exemplar sets\n","        exemplars = [[] for _ in range(10)]\n","\n","        # Iterate over classes\n","        for y in range(10):\n","            print(f\"Extracting exemplars from class {y} of current split... \", end=\"\")\n","\n","            # Transform samples to tensors and apply normalization\n","            transformed_samples = torch.zeros((len(samples[y]), 3, 32, 32)).to(self.device)\n","            for i in range(len(transformed_samples)):  \n","                transformed_samples[i] = self.test_transform(samples[y][i])\n","\n","            # Extract features from samples\n","            samples_features = self.extract_features(transformed_samples).to(self.device)\n","\n","            # Compute the feature mean of the current class\n","            features_mean = samples_features.mean(dim=0)\n","\n","            # Initializes indices vector, containing the index of each exemplar chosen\n","            idx = []\n","\n","            # See iCaRL algorithm 4\n","            for k in range(1, m+1): \n","                if k == 1: \n","                    f_sum = torch.zeros(64).to(self.device)\n","                else: \n","                    f_sum = samples_features[idx].sum(dim=0)\n","\n","                # Compute argument of argmin function\n","                f_arg = torch.norm(features_mean - 1/k * (samples_features + f_sum), dim=1) \n","                \n","\n","                # Mask exemplars that were already taken, as we do not want to store the\n","                # same exemplar more than once\n","                mask = np.zeros(len(f_arg), int)\n","                mask[idx] = 1\n","                f_arg_masked = ma.masked_array(f_arg.cpu().detach().numpy(), mask=mask) \n","\n","                # Compute the nearest available exemplar\n","                exemplar_idx = np.argmin(f_arg_masked)\n","\n","                idx.append(exemplar_idx)\n","            \n","            # Save exemplars to exemplar set\n","            for i in idx:\n","                exemplars[y].append(samples[y][i])\n","            \n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","            \n","        return exemplars\n","\n","    def reduce_exemplar_set(self, exemplar_set, m):\n","\n","        return exemplar_set[:m]\n","\n","    def train(self, train_dataset, val_dataset):\n","\n","        # Define the optimization algorithm\n","        parameters_to_optimize = self.net.parameters()\n","        self.optimizer = optim.SGD(parameters_to_optimize, \n","                                   lr=self.LR,\n","                                   momentum=self.MOMENTUM,\n","                                   weight_decay=self.WEIGHT_DECAY)\n","        \n","        # Define the learning rate decaying policy\n","        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n","                                                        milestones=self.MILESTONES,\n","                                                        gamma=self.GAMMA)\n","\n","        # Create DataLoaders for training and validation\n","        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","\n","        # Send networks to chosen device\n","        self.net = self.net.to(self.device)\n","        if self.old_net is not None: self.old_net = self.old_net.to(self.device)\n","\n","        cudnn.benchmark  # Calling this optimizes runtime\n","\n","        self.best_val_loss = float('inf')\n","        self.best_val_accuracy = 0\n","        self.best_train_loss = float('inf')\n","        self.best_train_accuracy = 0\n","        \n","        self.best_net = None\n","        self.best_epoch = -1\n","\n","        for epoch in range(self.NUM_EPOCHS):\n","            # Run an epoch (start counting form 1)\n","            train_loss, train_accuracy = self.do_epoch(epoch+1)\n","        \n","            # Validate after each epoch \n","            val_loss, val_accuracy = self.validate()    \n","\n","            # Validation criterion: best net is the one that minimizes the loss\n","            # on the validation set.\n","            if self.VALIDATE and val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.best_val_accuracy = val_accuracy\n","                self.best_train_loss = train_loss\n","                self.best_train_accuracy = train_accuracy\n","\n","                self.best_net = deepcopy(self.net)\n","                self.best_epoch = epoch\n","                print(\"Best model updated\")\n","\n","        if self.VALIDATE:\n","            val_loss = self.best_val_loss\n","            val_accuracy = self.best_val_accuracy\n","            train_loss = self.best_train_loss\n","            train_accuracy = self.best_train_accuracy\n","\n","            print(f\"Best model found at epoch {self.best_epoch+1}\")\n","\n","        return train_loss, train_accuracy, val_loss, val_accuracy\n","    \n","    def do_epoch(self, current_epoch):\n","\n","        # Set the current network in training mode\n","        self.net.train()\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_train_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n","\n","        for images, labels in self.train_dataloader:\n","            loss, corrects = self.do_batch(images, labels)\n","\n","            running_train_loss += loss.item()\n","            running_corrects += corrects\n","            total += labels.size(0)\n","            batch_idx += 1\n","\n","        self.scheduler.step()\n","\n","        # Calculate average scores\n","        train_loss = running_train_loss / batch_idx \n","        train_accuracy = running_corrects / float(total) \n","\n","        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n","\n","        return train_loss, train_accuracy\n","\n","    def do_batch(self, batch, labels):\n","\n","        batch = batch.to(self.device)\n","        labels = labels.to(self.device)\n","\n","        # Zero-ing the gradients\n","        self.optimizer.zero_grad()\n","\n","        num_classes = self.output_neurons_count() \n","        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n","\n","        if self.old_net is None:\n","\n","            targets = one_hot_labels\n","\n","        else:\n","            \n","            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10] \n","\n","            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)     \n","\n","        # Forward pass\n","        outputs = self.net(batch)\n","        loss = self.criterion(outputs, targets)\n","\n","        # Get predictions\n","        _, preds = torch.max(outputs.data, 1)\n","\n","        # Accuracy over new images\n","        running_corrects = torch.sum(preds == labels.data).data.item() \n","\n","        # Backward pass: computes gradients\n","        loss.backward()\n","\n","        self.optimizer.step()\n","\n","        return loss, running_corrects\n","\n","    def validate(self): \n","\n","        self.net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_val_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        for images, labels in self.val_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","            total += labels.size(0)\n","\n","            # One hot encoding of new task labels \n","            one_hot_labels = self.to_onehot(labels)\n","\n","            # New net forward pass\n","            outputs = self.net(images)  \n","            loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n","\n","            running_val_loss += loss.item()\n","\n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            # Update the number of correctly classified validation samples\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            batch_idx += 1\n","\n","        # Calculate scores\n","        val_loss = running_val_loss / batch_idx\n","        val_accuracy = running_corrects / float(total)\n","\n","        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n","\n","        return val_loss, val_accuracy\n","\n","    \n","    def increment_classes(self, n=10):\n","\n","        in_features = self.net.fc.in_features  \n","        out_features = self.net.fc.out_features  \n","        weight = self.net.fc.weight.data\n","        bias = self.net.fc.bias.data\n","\n","        self.net.fc = nn.Linear(in_features, out_features+n)\n","        self.net.fc.weight.data[:out_features] = weight\n","        self.net.fc.bias.data[:out_features] = bias\n","    \n","\n","    def output_neurons_count(self):\n","\n","        return self.net.fc.out_features\n","    \n","\n","    def feature_neurons_count(self):\n","\n","        return self.net.fc.in_features\n","    \n","\n","    def to_onehot(self, targets):\n"," \n","        num_classes = self.net.fc.out_features\n","        one_hot_targets = torch.eye(num_classes)[targets]\n","\n","        return one_hot_targets.to(self.device)\n"],"execution_count":null,"outputs":[]}]}