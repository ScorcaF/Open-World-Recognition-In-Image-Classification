{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"iCaRL_SVM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"fzwCT6oOEKHn"},"source":["################################################################################\n","# the following code was cleaned from comments that refers to iCaRL methods\n","# in order to increase cleanliness of the code. \n","# For further details on the implementation of iCaRL methods,\n","# check the commented version (iCaRL.ipynb)\n","#\n","################################################################################\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from torch.backends import cudnn\n","from torch.autograd import Variable\n","\n","import numpy as np\n","import numpy.ma as ma\n","from math import floor\n","from copy import deepcopy\n","import random\n","\n","from sklearn.svm import SVC\n","\n","\n","sigmoid = nn.Sigmoid() # Sigmoid function\n","softmax = nn.Softmax(dim=None)\n","logsoftmax = nn.LogSoftmax()\n","\n","class Exemplars(torch.utils.data.Dataset):\n","    def __init__(self, exemplars, transform=None):\n","    \n","        self.dataset = []\n","        self.targets = []\n","\n","        for y, exemplar_y in enumerate(exemplars):\n","            self.dataset.extend(exemplar_y)  \n","            self.targets.extend([y] * len(exemplar_y))  # return: [y,y,y,y, ... ] until len(exemplar_y)\n","\n","        self.transform = transform\n","    \n","    def __getitem__(self, index):\n","        image = self.dataset[index]\n","        target = self.targets[index]\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","\n","class iCaRL_svm:\n","\n","    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform, criterion, class_params, all_data):\n","        self.device = device\n","        self.net = net\n","\n","        # Set hyper-parameters\n","        self.LR = lr\n","        self.MOMENTUM = momentum\n","        self.WEIGHT_DECAY = weight_decay\n","        self.MILESTONES = milestones\n","        self.GAMMA = gamma\n","        self.NUM_EPOCHS = num_epochs\n","        self.BATCH_SIZE = batch_size\n","        \n","        # Set transformations\n","        self.train_transform = train_transform\n","        self.test_transform = test_transform\n","\n","        # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n","        # with num_classes the number of classes seen until now by the network.\n","        self.exemplars = []\n","\n","        # Initialize the copy of the old network, used to compute outputs of the\n","        # previous network for the distillation loss, to None. This is useful to\n","        # correctly apply the first function when training the network for the\n","        # first time.\n","        self.old_net = None\n","\n","        # Maximum number of exemplars\n","        self.memory_size = 2000\n","    \n","        # Loss function\n","        self.criterion = criterion\n","\n","        # If True, test on the best model found (e.g., minimize loss). If False,\n","        # test on the last model build (of the last epoch).\n","        self.VALIDATE = False\n","\n","        # classifier with parameters \n","        self.clf = SVC()\n","        self.parameters = class_params \n","        self.all_data = all_data\n","\n","\n","\n","    def classify(self, batch, train_dataset=None):\n","        \n","        batch_features = self.extract_features(batch) # (batch size, 64) \n","        for i in range(batch_features.size(0)):       \n","            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n","        batch_features = batch_features.to(self.device)\n","        \n","        ########################################\n","\n","        # train classifier part:\n","        # this part of code is executed just once since we need to train the classifier\n","        # just one time and not at each invokation of classify method\n","\n","        if self.X is None: #check if we already computed the training dataset for the classifier\n","             # start building the dataset\n","            self.X = []\n","            num_classes = len(self.exemplars)\n","            if train_dataset is not None and self.all_data == True:\n","                # initialize:\n","                # - X will contain current trainset + exemplars features\n","                # - y will contain the labels \n","                self.X = []\n","                self.y = []\n","                #####retrieve data from train_set (if we have it):#####\n","                for train_sample, label in train_dataset:                                     \n","                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform) \n","                    features = features/features.norm()\n","                    self.X.append(features.to(\"cpu\").numpy())\n","                    self.y.append(label)\n","\n","            #####retrieve exemplars and concatenate them ######       \n","            for y in range(num_classes):  \n","                for exemplar in self.exemplars[y]: \n","                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n","                    features = features/features.norm() # Normalize the feature representation of the exemplar\n","                    \n","                    self.X.append(features.to(\"cpu\").numpy())\n","                    self.y.append(y)\n","\n","\n","            if self.all_data == True:\n","              print('Training SVM classifier with all data...')\n","            else: \n","              print('Training SVM classifier with only exemplars...')\n","\n","            # Initialize classifier\n","            self.clf.set_params(**self.parameters)\n","            self.X = np.vstack(self.X)\n","            self.y = np.array(self.y)\n","            self.clf.fit(self.X,self.y)\n","            print(\"Classifier training ended. \")\n","  \n","        #########################################################\n","          \n","        preds = self.clf.predict(batch_features.to(\"cpu\").numpy())\n","        \n","        return torch.from_numpy(preds).to(\"cuda\")\n","    \n","    def extract_features(self, sample, batch=True, transform=None):\n"," \n","        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\"\n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        if batch is False: # Treat sample as single PIL image \n","            sample = transform(sample)\n","            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336, (3, 32, 32) --> (1, 3, 32, 32)\n","\n","        sample = sample.to(self.device)\n","\n","\n","        if self.VALIDATE: \n","            features = self.best_net.features(sample)\n","        else:\n","            features = self.net.features(sample)   \n","\n","        if batch is False:\n","            features = features[0] \n","\n","        return features\n","\n","    def incremental_train(self, split, train_dataset, val_dataset):\n","    \n","        if split is not 0:\n","\n","            self.increment_classes(10)\n","\n","  \n","        train_logs = self.update_representation(train_dataset, val_dataset)\n","   \n","        \n","  \n","        num_classes = self.output_neurons_count()\n","        m = floor(self.memory_size / num_classes)\n","\n","        print(f\"Target number of exemplars per class: {m}\")\n","        print(f\"Target total number of exemplars: {m*num_classes}\")\n","\n","        for y in range(len(self.exemplars)):\n","            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n","\n","  \n","        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m) \n","        self.exemplars.extend(new_exemplars)\n","\n","        return train_logs\n","\n","    def update_representation(self, train_dataset, val_dataset): \n","\n","        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n","\n","        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n","        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n","\n","        # Train the network on combined dataset\n","        train_logs = self.train(train_dataset_with_exemplars, val_dataset) \n","\n","        \n","        self.old_net = deepcopy(self.net)\n","\n","        return train_logs\n","\n","    def construct_exemplar_set_rand(self, dataset, m):\n","     \n","        dataset.dataset.disable_transform()\n","        \n","        #storing images of a split\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range: example: 21 -> 1, 22 -> 2 etc.\n","            samples[label].append(image)\n","\n","        # we can now turn back the transformation on train_dataset\n","        dataset.dataset.enable_transform()\n","\n","        exemplars = [[] for _ in range(10)]     \n","        #random sampling\n","        for y in range(10):\n","            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n","            # Randomly choose m samples from samples[y] without replacement\n","            exemplars[y] = random.sample(samples[y], m)\n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","\n","        return exemplars\n","\n","    def construct_exemplar_set_herding(self, dataset, m): \n","\n","        dataset.dataset.disable_transform()\n","\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range\n","            samples[label].append(image)\n","\n","        dataset.dataset.enable_transform()\n","\n","        # Initialize exemplar sets\n","        exemplars = [[] for _ in range(10)]\n","\n","        # Iterate over classes\n","        for y in range(10):\n","            print(f\"Extracting exemplars from class {y} of current split... \", end=\"\")\n","\n","            # Transform samples to tensors and apply normalization\n","            transformed_samples = torch.zeros((len(samples[y]), 3, 32, 32)).to(self.device)\n","            for i in range(len(transformed_samples)):  \n","                transformed_samples[i] = self.test_transform(samples[y][i])\n","\n","            # Extract features from samples\n","            samples_features = self.extract_features(transformed_samples).to(self.device)\n","\n","            # Compute the feature mean of the current class\n","            features_mean = samples_features.mean(dim=0)\n","\n","            # Initializes indices vector, containing the index of each exemplar chosen\n","            idx = []\n","\n","            # See iCaRL algorithm 4\n","            for k in range(1, m+1): # k = 1, ..., m -- Choose m exemplars\n","                if k == 1: # No exemplars chosen yet, sum to 0 vector\n","                    f_sum = torch.zeros(64).to(self.device)\n","                else: # Sum of features of all exemplars chosen until now (j = 1, ..., k-1)\n","                    f_sum = samples_features[idx].sum(dim=0)\n","\n","                # Compute argument of argmin function\n","                f_arg = torch.norm(features_mean - 1/k *(samples_features + f_sum), dim=1) \n","                \n","        \n","                mask = np.zeros(len(f_arg), int)\n","                mask[idx] = 1\n","                f_arg_masked = ma.masked_array(f_arg.cpu().detach().numpy(), mask=mask) \n","\n","                # Compute the nearest available exemplar\n","                exemplar_idx = np.argmin(f_arg_masked)\n","\n","                idx.append(exemplar_idx)\n","            \n","            # Save exemplars to exemplar set\n","            for i in idx:\n","                exemplars[y].append(samples[y][i])\n","            \n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","            \n","        return exemplars\n","\n","    def reduce_exemplar_set(self, exemplar_set, m):\n","   \n","        return exemplar_set[:m]\n","    \n","  \n","    def train(self, train_dataset, val_dataset):\n","     \n","        parameters_to_optimize = self.net.parameters()\n","        self.optimizer = optim.SGD(parameters_to_optimize, \n","                                   lr=self.LR,\n","                                   momentum=self.MOMENTUM,\n","                                   weight_decay=self.WEIGHT_DECAY)\n","        \n","        # Define the learning rate decaying policy\n","        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n","                                                        milestones=self.MILESTONES,\n","                                                        gamma=self.GAMMA)\n","\n","        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","\n","        self.net = self.net.to(self.device)\n","        if self.old_net is not None: self.old_net = self.old_net.to(self.device)\n","\n","        cudnn.benchmark  \n","        self.best_val_loss = float('inf')\n","        self.best_val_accuracy = 0\n","        self.best_train_loss = float('inf')\n","        self.best_train_accuracy = 0\n","        \n","        self.best_net = None\n","        self.best_epoch = -1\n","\n","        for epoch in range(self.NUM_EPOCHS):\n","           \n","            train_loss, train_accuracy = self.do_epoch(epoch+1)\n","        \n","            # Validate after each epoch \n","            val_loss, val_accuracy = self.validate()    \n","\n","            # Validation criterion: best net is the one that minimizes the loss\n","            # on the validation set.\n","            if self.VALIDATE and val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.best_val_accuracy = val_accuracy\n","                self.best_train_loss = train_loss\n","                self.best_train_accuracy = train_accuracy\n","\n","                self.best_net = deepcopy(self.net)\n","                self.best_epoch = epoch\n","                print(\"Best model updated\")\n","\n","        if self.VALIDATE:\n","            val_loss = self.best_val_loss\n","            val_accuracy = self.best_val_accuracy\n","            train_loss = self.best_train_loss\n","            train_accuracy = self.best_train_accuracy\n","\n","            print(f\"Best model found at epoch {self.best_epoch+1}\")\n","\n","        return train_loss, train_accuracy, val_loss, val_accuracy\n","    \n","    def do_epoch(self, current_epoch):\n","      \n","\n","        # Set the current network in training mode\n","        self.net.train()\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_train_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n","\n","        for images, labels in self.train_dataloader:\n","            loss, corrects = self.do_batch(images, labels)\n","\n","            running_train_loss += loss.item()\n","            running_corrects += corrects\n","            total += labels.size(0)\n","            batch_idx += 1\n","\n","        self.scheduler.step()\n","\n","        # Calculate average scores\n","        train_loss = running_train_loss / batch_idx # Average over all batches\n","        train_accuracy = running_corrects / float(total) # Average over all samples\n","\n","        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n","\n","        return train_loss, train_accuracy\n","\n","    def do_batch(self, batch, labels):\n","\n","        batch = batch.to(self.device)\n","        labels = labels.to(self.device)\n","\n","        # Zero-ing the gradients\n","        self.optimizer.zero_grad()\n","        \n","       \n","\n","        num_classes = self.output_neurons_count() # Number of classes seen until now, including new classes\n","        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n","\n","        if self.old_net is None:\n","            # Network is training for the first time, so we only apply the\n","            # classification loss.\n","            targets = one_hot_labels\n","\n","        else:\n","       \n","            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10] \n","\n","            \n","            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n","\n","        # Forward pass\n","        outputs = self.net(batch)\n","        loss = self.criterion(outputs, targets)\n","\n","        # Get predictions\n","        _, preds = torch.max(outputs.data, 1)\n","\n","        # Accuracy over NEW IMAGES, not over all images\n","        running_corrects = torch.sum(preds == labels.data).data.item() \n","\n","        # Backward pass: computes gradients\n","        loss.backward()\n","\n","        self.optimizer.step()\n","\n","        return loss, running_corrects\n","\n","    def validate(self): \n","\n","        self.net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_val_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        for images, labels in self.val_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","            total += labels.size(0)\n","\n","            # One hot encoding of new task labels \n","            one_hot_labels = self.to_onehot(labels)\n","\n","            # New net forward pass\n","            outputs = self.net(images)  \n","            loss = self.criterion(outputs, one_hot_labels) # loss type: BCE Loss with sigmoids over outputs\n","\n","            running_val_loss += loss.item()\n","\n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            # Update the number of correctly classified validation samples\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            batch_idx += 1\n","\n","        # Calculate scores\n","        val_loss = running_val_loss / batch_idx\n","        val_accuracy = running_corrects / float(total)\n","\n","        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n","\n","        return val_loss, val_accuracy\n","\n","    def test(self, test_dataset, train_dataset=None):\n","      \n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","        running_corrects = 0\n","        total = 0\n","\n","        # To store all predictions\n","        all_preds = torch.tensor([])\n","        all_preds = all_preds.type(torch.LongTensor)\n","        all_targets = torch.tensor([])\n","        all_targets = all_targets.type(torch.LongTensor)\n","\n","        # List on which train SVM for current split\n","        self.X = None \n","        \n","        # Disable transformations for train_dataset, if available, as we will\n","        # need original PIL images from which to extract features.\n","        if train_dataset is not None: train_dataset.dataset.disable_transform()\n","\n","        for images, labels in self.test_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","\n","            total += labels.size(0)\n","            \n","            with torch.no_grad():\n","                preds = self.classify(images, train_dataset)\n","\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            all_targets = torch.cat(\n","                (all_targets.to(self.device), labels.to(self.device)), dim=0\n","            )\n","\n","            all_preds = torch.cat(\n","                (all_preds.to(self.device), preds.to(self.device)), dim=0\n","            )\n","\n","        if train_dataset is not None: train_dataset.dataset.enable_transform()\n","\n","        # Calculate accuracy\n","        accuracy = running_corrects / float(total)  \n","\n","        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n","\n","        if train_dataset is None:\n","            print(\"(only exemplars)\")\n","        else:\n","            print(\"(exemplars and training data)\")\n","\n","        return accuracy, all_targets, all_preds\n","\n","    \n","    \n","    def increment_classes(self, n=10):\n","   \n","\n","        in_features = self.net.fc.in_features  # size of each input sample\n","        out_features = self.net.fc.out_features  # size of each output sample\n","        weight = self.net.fc.weight.data\n","        bias = self.net.fc.bias.data\n","\n","        self.net.fc = nn.Linear(in_features, out_features+n)\n","        self.net.fc.weight.data[:out_features] = weight\n","        self.net.fc.bias.data[:out_features] = bias\n","    \n","    def output_neurons_count(self):\n","\n","        return self.net.fc.out_features\n","    \n","\n","    \n","    def to_onehot(self, targets):\n","\n","        num_classes = self.net.fc.out_features\n","        one_hot_targets = torch.eye(num_classes)[targets]\n","\n","        return one_hot_targets.to(self.device)\n","\n","\n"],"execution_count":null,"outputs":[]}]}