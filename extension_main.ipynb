{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"extension_main.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"akp--Dj9RBTj"},"source":["#import libraries and package\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"cN4xvEWvpltx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkBUBd8bQ6HE"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n","\n","from torchvision import transforms\n","from torchvision.models import resnet34\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","from PIL import Image\n","from copy import deepcopy\n","\n","import numpy as np\n","import sys\n","import os\n","np.set_printoptions(threshold=sys.maxsize) #needed to print correctly the logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_OSaUaoksjR"},"source":["righe di codice utili per importare da altri jupyter notebook delle funzioni. "]},{"cell_type":"code","metadata":{"id":"i2MXsnVKxbzx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626193915861,"user_tz":-120,"elapsed":17479,"user":{"displayName":"FRANCESCO SCORCA","photoUrl":"","userId":"11778786325269687848"}},"outputId":"1956422d-24f3-4c7a-cc16-9a6de3359af0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TrWrAj7bjtv5"},"source":["%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto\"\n","!pip install import-ipynb\n","\n","import import_ipynb "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKzmzOrAkGiM"},"source":["#retrieve from folder 'data'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/data\"                          \n","from DataClean import Cifar100 as datasetManager   # from **nome jupyter notebook** import **classe o metodo da importare**\n","\n","#retrieve from folder 'models'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/models\"\n","from resnet import resnet32\n","from Manager import Manager\n","\n","#retrieve from folder 'extension'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/extension\"\n","from iCaRL_extension import iCaRL_ext\n","\n","#retrieve from folder 'logs'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/logs\"\n","from Save_logs import save_logs\n","\n","# return in the main project folder\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vl63oUufRbJN"},"source":["DEVICE=torch.device(\"cuda:0\")\n","DIR='/content/data'\n","\n","RANDOM_SEED = [1993,30,423]    \n","NUM_CLASSES = 100       # Total number of classes\n","\n","# Training\n","BATCH_SIZE = 64         # Batch size \n","NUM_EPOCHS = 70         # Total number of training epochs\n","LR = 2                  # Initial learning rate\n","MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n","WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n","MILESTONES = [49, 63]   # Step down policy from iCaRL (MultiStepLR)\n","                        # Decrease the learning rate by gamma at each milestone\n","GAMMA = 0.2             # Gamma factor from iCaRL\n","\n","\n","### define dataset transformation ###\n","train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),  #useful data augmentation\n","                                      transforms.RandomHorizontalFlip(),     #useful data augmentation\n","                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n","                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","test_transform = transforms.Compose([transforms.ToTensor(),\n","                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ufohk-zw87no"},"source":["# iCaRL extension"]},{"cell_type":"code","metadata":{"id":"zr1zantAWT4o"},"source":["RUN_NAME = 'iCarl_extension'\n","\n","CUDA_LAUNCH_BLOCKING=1\n","NUM_RUNS = len(RANDOM_SEED)\n","\n","#### customize hyperparameter for tuning ######\n","############ (debug purpose) ##################\n","LR = 0.1\n","NUM_EPOCHS = 70\n","WEIGHT_DECAY = 1e-4\n","###############################################\n","\n","\n","# Initialize logs\n","logs = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","   #download cifar100:\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","\n","    \n","  net = resnet32()\n","  icarl = iCaRL_ext(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n","\n","  for split_i in range(10):\n","        print(f\"-- Split {split_i} of run {run_i} (SEED: {RANDOM_SEED[run_i]}) --\")\n","\n","        ############## 10-class split management: ##################\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","        ############## extraction of validation index ##############\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   #choose here the test size\n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)                        #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)                            #the original train_dataset\n","\n","\n","        ############## training + log generation ###################        \n","        icarl.incremental_train(split_i, train_data_split, val_data_split)\n","        \n","        logs[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","\n","save_logs(logs_icarl_herd,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzDHw7j1qS8I"},"source":["# Tests on semantic drift compensation"]},{"cell_type":"code","metadata":{"id":"J8TXLRjlm6A_"},"source":["from iCaRL_extension import iCaRL_SDC\n","torch.autograd.set_detect_anomaly(True)\n","\n","RUN_NAME = 'icarl_SDC'\n","\n","NUM_RUNS = len(RANDOM_SEED)\n","NUM_EPOCHS = 70\n","\n","\n","logs = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","\n","    \n","  net = resnet32()\n","  icarl = iCaRL_SDC(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n","\n","  for split_i in range(10):\n","        print(f\"## Split {split_i} of run {run_i} ##\")\n","\n","\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      \n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   \n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)                        \n","        val_data_split = Subset(train_dataset , val_idx)                            \n","        \n","        train_logs = icarl.incremental_train(split_i, train_data_split, val_data_split)\n","        \n","        \n","\n","\n","        logs[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","\n","        logs[run_i][split_i]['accuracy'] = acc\n","        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","\n","save_logs(logs,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBWgV8O7eOCc"},"source":[""],"execution_count":null,"outputs":[]}]}