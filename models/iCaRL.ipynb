{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"iCaRL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"TpgD-mP78I48"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from torch.backends import cudnn\n","from torch.autograd import Variable\n","\n","import numpy as np\n","import numpy.ma as ma\n","from math import floor\n","from copy import deepcopy\n","import random\n","\n","\n","sigmoid = nn.Sigmoid() # Sigmoid function\n","\n","class Exemplars(torch.utils.data.Dataset):\n","    def __init__(self, exemplars, transform=None):\n","        # exemplars = [\n","        #     [ex0_class0, ex1_class0, ex2_class0, ...],\n","        #     [ex0_class1, ex1_class1, ex2_class1, ...],\n","        #     ...\n","        #     [ex0_classN, ex1_classN, ex2_classN, ...]\n","        # ]\n","\n","        self.dataset = []\n","        self.targets = []\n","\n","        for y, exemplar_y in enumerate(exemplars):\n","            self.dataset.extend(exemplar_y)             \n","            self.targets.extend([y] * len(exemplar_y))  # return: [y,y,y,y, ... ] until len(exemplar_y)\n","\n","        self.transform = transform\n","    \n","    def __getitem__(self, index):\n","        image = self.dataset[index]\n","        target = self.targets[index]\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","class iCaRL_rand():   \n","    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform):\n","        self.device = device\n","        self.net = net\n","\n","        # Set hyper-parameters\n","        self.LR = lr\n","        self.MOMENTUM = momentum\n","        self.WEIGHT_DECAY = weight_decay\n","        self.MILESTONES = milestones\n","        self.GAMMA = gamma\n","        self.NUM_EPOCHS = num_epochs\n","        self.BATCH_SIZE = batch_size\n","        \n","        # Set transformations\n","        self.train_transform = train_transform\n","        self.test_transform = test_transform\n","\n","        # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n","        # with num_classes the number of classes seen until now by the network.\n","        self.exemplars = []\n","\n","        # Initialize the copy of the old network, used to compute outputs of the\n","        # previous network for the distillation loss, to None. This is useful to\n","        # correctly apply the first function when training the network for the\n","        # first time.\n","        self.old_net = None\n","\n","        # Maximum number of exemplars\n","        self.memory_size = 2000\n","    \n","        # Loss function\n","        self.criterion = nn.BCEWithLogitsLoss()\n","\n","        # If True, test on the best model found (e.g., minimize loss). If False,\n","        # test on the last model build (of the last epoch).\n","        self.VALIDATE = False\n","\n","    def classify(self, batch, train_dataset=None):\n","\n","        batch_features = self.extract_features(batch) \n","        for i in range(batch_features.size(0)):       \n","            batch_features[i] = batch_features[i]/batch_features[i].norm()      # Normalize sample feature representation\n","        batch_features = batch_features.to(self.device)\n","        \n","        #retrieve prototipes\n","        if self.cached_means is None:\n","            print(\"Computing mean of exemplars... \", end=\"\")\n","\n","            self.cached_means = []\n","\n","            # Number of known classes\n","            num_classes = len(self.exemplars)  \n","\n","            # Compute the means of classes with all the data available,\n","            # including training data which contains samples belonging to\n","            # the latest 10 classes. This will remove noise from the mean\n","            # estimate, improving the results.\n","            if train_dataset is not None:\n","                train_features_list = [[] for _ in range(10)]\n","\n","\n","               #retrieve whole train_dataset\n","                for train_sample, label in train_dataset: \n","                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform) \n","                    features = features/features.norm()\n","                    train_features_list[label % 10].append(features)\n","\n","            # Compute means of exemplars for all known classes\n","            for y in range(num_classes):\n","                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)):\n","                    features_list = train_features_list[y % 10]\n","                else:\n","                    features_list = []\n","                \n","\n","                for exemplar in self.exemplars[y]: \n","                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n","                    features = features/features.norm() # Normalize the feature representation of the exemplar\n","                    features_list.append(features)      \n","                \n","                features_list = torch.stack(features_list)         \n","                class_means = features_list.mean(dim=0) \n","                class_means = class_means/class_means.norm() # Normalize the class means\n","\n","\n","                self.cached_means.append(class_means)\n","            \n","            self.cached_means = torch.stack(self.cached_means).to(self.device)\n","            print(\"done\")\n","        \n","        #Classifier:\n","        preds = []\n","        for i in range(batch_features.size(0)): \n","            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1) \n","            preds.append(torch.argmin(f_arg)) \n","        \n","        return torch.stack(preds)\n","    \n","    def extract_features(self, sample, batch=True, transform=None):\n","\n","        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\" \n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        if batch is False:               # Treat sample as single PIL image \n","            sample = transform(sample)\n","            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336, (3, 32, 32) --> (1, 3, 32, 32)\n","\n","        sample = sample.to(self.device)\n","       \n","\n","        if self.VALIDATE: \n","            features = self.best_net.features(sample)\n","        else:\n","            features = self.net.features(sample)   \n","\n","        if batch is False:\n","            features = features[0] \n","\n","        return features\n","\n","    def incremental_train(self, split, train_dataset, val_dataset):\n","\n","        if split is not 0:\n","            # Increment the number of output nodes for the new network by 10\n","            #starting from 1 (at run 0 we already have 10 output nodes)\n","            self.increment_classes(10)\n","\n","        # Improve network parameters upon receiving new classes. Effectively\n","        # train a new network starting from the current network parameters.\n","\n","        train_logs = self.update_representation(train_dataset, val_dataset)         \n","        \n","        #once trained on:\n","        # - all available exemplars\n","        # - all train_dataset\n","        # we need to make the new exemplar set for the 10 new classes\n","\n","        num_classes = self.output_neurons_count()\n","        m = floor(self.memory_size / num_classes)\n","\n","        print(f\"Target number of exemplars per class: {m}\")\n","        print(f\"Target total number of exemplars: {m*num_classes}\")\n","\n","        # Reduce pre-existing exemplar sets in order to fit new exemplars\n","        for y in range(len(self.exemplars)):\n","            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n","\n","        # Construct exemplar set for new classes: \n","        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m) #random construction\n","        self.exemplars.extend(new_exemplars)\n","\n","        return train_logs\n","\n","    # iCaRL algorithm 3\n","    def update_representation(self, train_dataset, val_dataset): \n","\n","        # Combine the new training data with existing exemplars and train the model     \n","\n","        # len(self.exemplars) = num seen classes\n","        # len(self.exemplars[y]) = num element of a classes\n","        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n","\n","        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n","        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n","\n","        # Train the network on combined dataset\n","        train_logs = self.train(train_dataset_with_exemplars, val_dataset) \n","\n","        # Keep a copy of the current network in order to compute its outputs for\n","        # the distillation loss while the new network is being trained.\n","        self.old_net = deepcopy(self.net)\n","\n","        return train_logs\n","\n","    def construct_exemplar_set_rand(self, dataset, m):\n","\n","        # the exemplar must be stored as image, so we disable transformation\n","        # that applied to the train_dataset (train_transformer is applied on train_dataset)\n","        # in order to train the model \n","        dataset.dataset.disable_transform()\n","        \n","        #storing images of a split\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range: example: 21 -> 1, 22 -> 2 etc.\n","            samples[label].append(image)\n","\n","        # we can now turn back the transformation on train_dataset\n","        dataset.dataset.enable_transform()\n","\n","        exemplars = [[] for _ in range(10)]     \n","        #random sampling\n","        for y in range(10):\n","            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n","            # Randomly choose m samples from samples[y] without replacement\n","            exemplars[y] = random.sample(samples[y], m)\n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","\n","        return exemplars\n","\n","\n","    def reduce_exemplar_set(self, exemplar_set, m):\n","\n","        return exemplar_set[:m]\n","    \n","    #\n","    #train is the same of standard train routine\n","    #\n","    def train(self, train_dataset, val_dataset):\n","        # Define the optimization algorithm\n","        parameters_to_optimize = self.net.parameters()\n","        self.optimizer = optim.SGD(parameters_to_optimize, \n","                                   lr=self.LR,\n","                                   momentum=self.MOMENTUM,\n","                                   weight_decay=self.WEIGHT_DECAY)\n","        \n","        # Define the learning rate decaying policy\n","        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n","                                                        milestones=self.MILESTONES,\n","                                                        gamma=self.GAMMA)\n","\n","        # Create DataLoaders for training and validation\n","        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","\n","        # Send networks to chosen device\n","        self.net = self.net.to(self.device)\n","        if self.old_net is not None: self.old_net = self.old_net.to(self.device)\n","\n","        cudnn.benchmark  # Calling this optimizes runtime\n","\n","        self.best_val_loss = float('inf')\n","        self.best_val_accuracy = 0\n","        self.best_train_loss = float('inf')\n","        self.best_train_accuracy = 0\n","        \n","        self.best_net = None\n","        self.best_epoch = -1\n","\n","        for epoch in range(self.NUM_EPOCHS):\n","            # Run an epoch (start counting form 1)\n","            train_loss, train_accuracy = self.do_epoch(epoch+1)\n","        \n","            # Validate after each epoch \n","            val_loss, val_accuracy = self.validate()    \n","\n","            # Validation criterion: best net is the one that minimizes the loss\n","            # on the validation set.\n","            if self.VALIDATE and val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.best_val_accuracy = val_accuracy\n","                self.best_train_loss = train_loss\n","                self.best_train_accuracy = train_accuracy\n","\n","                self.best_net = deepcopy(self.net)\n","                self.best_epoch = epoch\n","                print(\"Best model updated\")\n","\n","        if self.VALIDATE:\n","            val_loss = self.best_val_loss\n","            val_accuracy = self.best_val_accuracy\n","            train_loss = self.best_train_loss\n","            train_accuracy = self.best_train_accuracy\n","\n","            print(f\"Best model found at epoch {self.best_epoch+1}\")\n","\n","        return train_loss, train_accuracy, val_loss, val_accuracy\n","    \n","    def do_epoch(self, current_epoch):\n","\n","        # Set the current network in training mode\n","        self.net.train()\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_train_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n","\n","        for images, labels in self.train_dataloader:\n","            loss, corrects = self.do_batch(images, labels)\n","\n","            running_train_loss += loss.item()\n","            running_corrects += corrects\n","            total += labels.size(0)\n","            batch_idx += 1\n","\n","        self.scheduler.step()\n","\n","        # Calculate average scores\n","        train_loss = running_train_loss / batch_idx # Average over all batches\n","        train_accuracy = running_corrects / float(total) # Average over all samples\n","\n","        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n","\n","        return train_loss, train_accuracy\n","\n","    def do_batch(self, batch, labels):\n","\n","        batch = batch.to(self.device)\n","        labels = labels.to(self.device)\n","\n","        # Zero-ing the gradients\n","        self.optimizer.zero_grad()\n","        \n","        num_classes = self.output_neurons_count()                              # Number of classes seen until now, including new classes\n","        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes] # we take just the new classes info\n","\n","        if self.old_net is None:\n","            # Network is training for the first time, so we only apply the\n","            # classification loss.\n","            targets = one_hot_labels\n","\n","        else:\n","            # Old net forward pass. We compute the outputs of the old network\n","            # and apply a sigmoid function. These are used in the distillation\n","            # loss. We discard the output of the new neurons, as they are not\n","            # considered in the distillation loss. \n","            \n","            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10] \n","\n","            # Concatenate the outputs of the old network and the one-hot encoded\n","            # labels along dimension 1 (columns).\n","            # \n","            # Each row refers to an image in the training set, and contains:\n","            # - the output of the old network for that image, used by the\n","            #   distillation loss\n","            # - the one-hot label of the image, used by the classification loss\n","            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n","\n","        # Forward pass\n","        outputs = self.net(batch)\n","        loss = self.criterion(outputs, targets)\n","\n","        # Get predictions\n","        _, preds = torch.max(outputs.data, 1)\n","\n","        # Accuracy over NEW IMAGES, not over all images\n","        running_corrects = torch.sum(preds == labels.data).data.item() \n","\n","        # Backward pass: computes gradients\n","        loss.backward()\n","\n","        self.optimizer.step()\n","\n","        return loss, running_corrects\n","\n","    def validate(self):\n","\n","        self.net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_val_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        for images, labels in self.val_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","            total += labels.size(0)\n","\n","            # One hot encoding of new task labels \n","            one_hot_labels = self.to_onehot(labels)\n","\n","            # New net forward pass\n","            outputs = self.net(images)  \n","            loss = self.criterion(outputs, one_hot_labels) # loss type: BCE Loss with sigmoids over outputs\n","\n","            running_val_loss += loss.item()\n","\n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            # Update the number of correctly classified validation samples\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            batch_idx += 1\n","\n","        # Calculate scores\n","        val_loss = running_val_loss / batch_idx\n","        val_accuracy = running_corrects / float(total)\n","\n","        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n","\n","        return val_loss, val_accuracy\n","\n","    def test(self, test_dataset, train_dataset=None):\n","\n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","        running_corrects = 0\n","        total = 0\n","\n","        # To store all predictions\n","        all_preds = torch.tensor([])\n","        all_preds = all_preds.type(torch.LongTensor)\n","        all_targets = torch.tensor([])\n","        all_targets = all_targets.type(torch.LongTensor)\n","\n","        # Clear mean of exemplars cache\n","        self.cached_means = None #ogni volta che si testa la rete vanno ricomputati i prototipes: i parametri della rete sono cambiati dopo il training --> cambieranno le rappresentazioni delle immagini\n","        \n","        # Disable transformations for train_dataset, if available, as we will\n","        # need original PIL images from which to extract features.\n","        if train_dataset is not None: train_dataset.dataset.disable_transform()\n","\n","        for images, labels in self.test_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","\n","            total += labels.size(0)\n","            \n","            with torch.no_grad():\n","                preds = self.classify(images, train_dataset)\n","\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            all_targets = torch.cat(\n","                (all_targets.to(self.device), labels.to(self.device)), dim=0\n","            )\n","\n","            all_preds = torch.cat(\n","                (all_preds.to(self.device), preds.to(self.device)), dim=0\n","            )\n","\n","        if train_dataset is not None: train_dataset.dataset.enable_transform()\n","\n","        # Calculate accuracy\n","        accuracy = running_corrects / float(total)  \n","\n","        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n","\n","        if train_dataset is None:\n","            print(\"(only exemplars)\")\n","        else:\n","            print(\"(exemplars and training data)\")\n","\n","        return accuracy, all_targets, all_preds\n","\n","    \n","    def increment_classes(self, n=10):\n","\n","        in_features = self.net.fc.in_features  # size of each input sample\n","        out_features = self.net.fc.out_features  # size of each output sample\n","        weight = self.net.fc.weight.data\n","        bias = self.net.fc.bias.data\n","\n","        self.net.fc = nn.Linear(in_features, out_features+n)\n","        self.net.fc.weight.data[:out_features] = weight\n","        self.net.fc.bias.data[:out_features] = bias\n","    \n","    def output_neurons_count(self):\n","        return self.net.fc.out_features\n","    \n","    \n","    def to_onehot(self, targets):\n","        num_classes = self.net.fc.out_features\n","        one_hot_targets = torch.eye(num_classes)[targets]\n","\n","        return one_hot_targets.to(self.device)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_436Hn4nsYSI"},"source":["class iCaRL_herd:\n","\n","\n","    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform):\n","        self.device = device\n","        self.net = net\n","\n","        # Set hyper-parameters\n","        self.LR = lr\n","        self.MOMENTUM = momentum\n","        self.WEIGHT_DECAY = weight_decay\n","        self.MILESTONES = milestones\n","        self.GAMMA = gamma\n","        self.NUM_EPOCHS = num_epochs\n","        self.BATCH_SIZE = batch_size\n","        \n","        # Set transformations\n","        self.train_transform = train_transform\n","        self.test_transform = test_transform\n","\n","        # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n","        # with num_classes the number of classes seen until now by the network.\n","        self.exemplars = []\n","\n","        # Initialize the copy of the old network, used to compute outputs of the\n","        # previous network for the distillation loss, to None. This is useful to\n","        # correctly apply the first function when training the network for the\n","        # first time.\n","        self.old_net = None\n","\n","        # Maximum number of exemplars\n","        self.memory_size = 2000\n","    \n","        # Loss function\n","        self.criterion = nn.BCEWithLogitsLoss()\n","\n","        # If True, test on the best model found (e.g., minimize loss). If False,\n","        # test on the last model build (of the last epoch).\n","        self.VALIDATE = False\n","\n","    def classify(self, batch, train_dataset=None): \n","        #Computing features for batch to classify\n","        batch_features = self.extract_features(batch) # (batch size, 64) \n","        for i in range(batch_features.size(0)):       \n","            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n","        batch_features = batch_features.to(self.device)\n","        \n","        #Computing prototipes\n","        if self.cached_means is None:\n","            print(\"Computing mean of exemplars... \", end=\"\")\n","\n","            self.cached_means = []\n","\n","            # Number of known classes\n","            num_classes = len(self.exemplars)\n","\n","            # Compute the means of classes with all the data available,\n","            # including training data which contains samples belonging to\n","            # the latest 10 classes. This will remove noise from the mean\n","            # estimate, improving the results.  \n","            if train_dataset is not None:\n","                train_features_list = [[] for _ in range(10)]\n","\n","\n","              \n","                for train_sample, label in train_dataset: \n","                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform)\n","                    features = features/features.norm()\n","                    train_features_list[label % 10].append(features)\n","\n","            # Compute means of exemplars for all known classes\n","\n","            for y in range(num_classes):\n","                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)): #se label di una nuova classe\n","                    features_list = train_features_list[y % 10]\n","                else:\n","                    features_list = []\n","           \n","                for exemplar in self.exemplars[y]: \n","                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n","                    features = features/features.norm() \n","                    features_list.append(features)\n","                \n","                features_list = torch.stack(features_list)\n","                class_means = features_list.mean(dim=0) \n","                class_means = class_means/class_means.norm() # Normalize the class means\n","\n","                self.cached_means.append(class_means)\n","            \n","            self.cached_means = torch.stack(self.cached_means).to(self.device)\n","            print(\"done\")\n","        \n","        \n","        preds = []\n","        for i in range(batch_features.size(0)): \n","            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1)\n","            preds.append(torch.argmin(f_arg)) \n","        \n","        return torch.stack(preds)\n","    \n","    def extract_features(self, sample, batch=True, transform=None):\n","     \n","        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\" \n","\n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        if batch is False: # Treat sample as single PIL image\n","            sample = transform(sample)\n","            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336, (3, 32, 32) --> (1, 3, 32, 32)\n","\n","        sample = sample.to(self.device)\n","\n","        if self.VALIDATE: \n","            features = self.best_net.features(sample)\n","        else:\n","            features = self.net.features(sample)\n","\n","        if batch is False:\n","            features = features[0] \n","        return features\n","\n","    def incremental_train(self, split, train_dataset, val_dataset):\n","      \n","        if split is not 0:\n","            # Increment the number of output nodes for the new network by 10\n","            self.increment_classes(10)\n","\n","        # Improve network parameters upon receiving new classes. Effectively\n","        # train a new network starting from the current network parameters.\n","        train_logs = self.update_representation(train_dataset, val_dataset)\n","\n","        # Compute the number of exemplars per class\n","        num_classes = self.output_neurons_count()\n","        m = floor(self.memory_size / num_classes)\n","\n","        print(f\"Target number of exemplars per class: {m}\")\n","        print(f\"Target total number of exemplars: {m*num_classes}\")\n","\n","        # Reduce pre-existing exemplar sets in order to fit new exemplars\n","        for y in range(len(self.exemplars)):\n","            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n","\n","        # Construct exemplar set for new classes\n","        new_exemplars = self.construct_exemplar_set_herding(train_dataset, m) \n","        self.exemplars.extend(new_exemplars)\n","\n","        return train_logs\n","\n","    def update_representation(self, train_dataset, val_dataset):\n"," \n","\n","        # Combine the new training data with existing exemplars.\n"," \n","        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n","\n","        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n","        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n","\n","        # Train the network on combined dataset\n","        train_logs = self.train(train_dataset_with_exemplars, val_dataset) \n","\n","        # Keep a copy of the current network in order to compute its outputs for\n","        # the distillation loss while the new network is being trained.\n","        self.old_net = deepcopy(self.net)\n","\n","        return train_logs\n","\n","    def construct_exemplar_set_herding(self, dataset, m):\n","\n","        dataset.dataset.disable_transform()\n","\n","        samples = [[] for _ in range(10)]\n","        for image, label in dataset:\n","            label = label % 10 # Map labels to 0-9 range\n","            samples[label].append(image)\n","\n","        dataset.dataset.enable_transform()\n","\n","        # Initialize exemplar sets\n","        exemplars = [[] for _ in range(10)]\n","\n","        # Iterate over classes\n","        for y in range(10):\n","            print(f\"Extracting exemplars by herding from class {y} of current split... \", end=\"\")\n","\n","            # Transform samples to tensors and apply normalization\n","            transformed_samples = torch.zeros((len(samples[y]), 3, 32, 32)).to(self.device)\n","            for i in range(len(transformed_samples)):  \n","                transformed_samples[i] = self.test_transform(samples[y][i])\n","\n","            # Extract features from samples\n","            samples_features = self.extract_features(transformed_samples).to(self.device)\n","\n","            # Compute the feature mean of the current class\n","            features_mean = samples_features.mean(dim=0)\n","\n","            # Initializes indices vector, containing the index of each exemplar chosen\n","            idx = []\n","\n","            # See iCaRL algorithm 4\n","            for k in range(1, m+1): # k = 1, ..., m -- Choose m exemplars\n","                if k == 1: # No exemplars chosen yet, sum to 0 vector\n","                    f_sum = torch.zeros(64).to(self.device)\n","                else: # Sum of features of all exemplars chosen until now (j = 1, ..., k-1)\n","                    f_sum = samples_features[idx].sum(dim=0)\n","\n","                # Compute argument of argmin function\n","                f_arg = torch.norm(features_mean - 1/k * (samples_features + f_sum), dim=1)\n","               \n","\n","                # Mask exemplars that were already taken, as we do not want to store the\n","                # same exemplar more than once\n","                mask = np.zeros(len(f_arg), int)\n","                mask[idx] = 1\n","                f_arg_masked = ma.masked_array(f_arg.cpu().detach().numpy(), mask=mask) \n","\n","                # Compute the nearest available exemplar\n","                exemplar_idx = np.argmin(f_arg_masked)\n","\n","                idx.append(exemplar_idx)\n","            \n","            # Save exemplars to exemplar set\n","            for i in idx:\n","                exemplars[y].append(samples[y][i])\n","            \n","            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n","            \n","        return exemplars\n","\n","    def reduce_exemplar_set(self, exemplar_set, m):\n","\n","        return exemplar_set[:m]\n","\n","    def train(self, train_dataset, val_dataset):\n","\n","        # Define the optimization algorithm\n","        parameters_to_optimize = self.net.parameters()\n","        self.optimizer = optim.SGD(parameters_to_optimize, \n","                                   lr=self.LR,\n","                                   momentum=self.MOMENTUM,\n","                                   weight_decay=self.WEIGHT_DECAY)\n","        \n","        # Define the learning rate decaying policy\n","        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n","                                                        milestones=self.MILESTONES,\n","                                                        gamma=self.GAMMA)\n","\n","        # Create DataLoaders for training and validation\n","        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","\n","        # Send networks to chosen device\n","        self.net = self.net.to(self.device)\n","        if self.old_net is not None: self.old_net = self.old_net.to(self.device)\n","\n","        cudnn.benchmark  # Calling this optimizes runtime\n","\n","        self.best_val_loss = float('inf')\n","        self.best_val_accuracy = 0\n","        self.best_train_loss = float('inf')\n","        self.best_train_accuracy = 0\n","        \n","        self.best_net = None\n","        self.best_epoch = -1\n","\n","        for epoch in range(self.NUM_EPOCHS):\n","            # Run an epoch (start counting form 1)\n","            train_loss, train_accuracy = self.do_epoch(epoch+1)\n","        \n","            # Validate after each epoch \n","            val_loss, val_accuracy = self.validate()    \n","\n","            # Validation criterion: best net is the one that minimizes the loss\n","            # on the validation set.\n","            if self.VALIDATE and val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.best_val_accuracy = val_accuracy\n","                self.best_train_loss = train_loss\n","                self.best_train_accuracy = train_accuracy\n","\n","                self.best_net = deepcopy(self.net)\n","                self.best_epoch = epoch\n","                print(\"Best model updated\")\n","\n","        if self.VALIDATE:\n","            val_loss = self.best_val_loss\n","            val_accuracy = self.best_val_accuracy\n","            train_loss = self.best_train_loss\n","            train_accuracy = self.best_train_accuracy\n","\n","            print(f\"Best model found at epoch {self.best_epoch+1}\")\n","\n","        return train_loss, train_accuracy, val_loss, val_accuracy\n","    \n","    def do_epoch(self, current_epoch):\n","    \n","        # Set the current network in training mode\n","        self.net.train()\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_train_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n","\n","        for images, labels in self.train_dataloader:\n","            loss, corrects = self.do_batch(images, labels)\n","\n","            running_train_loss += loss.item()\n","            running_corrects += corrects\n","            total += labels.size(0)\n","            batch_idx += 1\n","\n","        self.scheduler.step()\n","\n","        # Calculate average scores\n","        train_loss = running_train_loss / batch_idx # Average over all batches\n","        train_accuracy = running_corrects / float(total) # Average over all samples\n","\n","        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n","\n","        return train_loss, train_accuracy\n","\n","    def do_batch(self, batch, labels):\n"," \n","        batch = batch.to(self.device)\n","        labels = labels.to(self.device)\n","\n","        # Zero-ing the gradients\n","        self.optimizer.zero_grad()\n","   \n","\n","        num_classes = self.output_neurons_count() # Number of classes seen until now, including new classes\n","        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n","\n","        if self.old_net is None:\n","            # Network is training for the first time, so we only apply the\n","            # classification loss.\n","            targets = one_hot_labels\n","\n","        else:\n","            # Old net forward pass. We compute the outputs of the old network\n","            # and apply a sigmoid function. These are used in the distillation\n","            # loss. We discard the output of the new neurons, as they are not\n","            # considered in the distillation loss.\n","            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10] \n","\n","            # Concatenate the outputs of the old network and the one-hot encoded\n","            # labels along dimension 1 (columns).\n","            # \n","            # Each row refers to an image in the training set, and contains:\n","            # - the output of the old network for that image, used by the\n","            #   distillation loss\n","            # - the one-hot label of the image, used by the classification loss\n","            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n","\n","        # Forward pass\n","        outputs = self.net(batch)\n","        loss = self.criterion(outputs, targets)\n","\n","        # Get predictions\n","        _, preds = torch.max(outputs.data, 1)\n","\n","        # Accuracy over NEW IMAGES, not over all images\n","        running_corrects = torch.sum(preds == labels.data).data.item() \n","\n","        # Backward pass: computes gradients\n","        loss.backward()\n","\n","        self.optimizer.step()\n","\n","        return loss, running_corrects\n","\n","    def validate(self):\n","\n","        self.net.train(False)\n","        if self.old_net is not None: self.old_net.train(False)\n","        if self.best_net is not None: self.best_net.train(False)\n","\n","        running_val_loss = 0\n","        running_corrects = 0\n","        total = 0\n","        batch_idx = 0\n","\n","        for images, labels in self.val_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","            total += labels.size(0)\n","\n","            # One hot encoding of new task labels \n","            one_hot_labels = self.to_onehot(labels)\n","\n","            # New net forward pass\n","            outputs = self.net(images)  \n","            loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n","\n","            running_val_loss += loss.item()\n","\n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            # Update the number of correctly classified validation samples\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            batch_idx += 1\n","\n","        # Calculate scores\n","        val_loss = running_val_loss / batch_idx\n","        val_accuracy = running_corrects / float(total)\n","\n","        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n","\n","        return val_loss, val_accuracy\n","\n","    def test(self, test_dataset, train_dataset=None):\n","  \n","        self.net.train(False)\n","        if self.best_net is not None: self.best_net.train(False) \n","        if self.old_net is not None: self.old_net.train(False)\n","\n","        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","        running_corrects = 0\n","        total = 0\n","\n","        # To store all predictions\n","        all_preds = torch.tensor([])\n","        all_preds = all_preds.type(torch.LongTensor)\n","        all_targets = torch.tensor([])\n","        all_targets = all_targets.type(torch.LongTensor)\n","\n","        # Clear mean of exemplars cache\n","        self.cached_means = None\n","        \n","        # Disable transformations for train_dataset, if available, as we will\n","        # need original PIL images from which to extract features.\n","        if train_dataset is not None: train_dataset.dataset.disable_transform()\n","\n","        for images, labels in self.test_dataloader:\n","            images = images.to(self.device)\n","            labels = labels.to(self.device)\n","\n","            total += labels.size(0)\n","            \n","            with torch.no_grad():\n","                preds = self.classify(images, train_dataset)\n","\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            all_targets = torch.cat(\n","                (all_targets.to(self.device), labels.to(self.device)), dim=0\n","            )\n","\n","            all_preds = torch.cat(\n","                (all_preds.to(self.device), preds.to(self.device)), dim=0\n","            )\n","\n","        if train_dataset is not None: train_dataset.dataset.enable_transform()\n","\n","        # Calculate accuracy\n","        accuracy = running_corrects / float(total)  \n","\n","        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n","\n","        if train_dataset is None:\n","            print(\"(only exemplars)\")\n","        else:\n","            print(\"(exemplars and training data)\")\n","\n","        return accuracy, all_targets, all_preds\n","\n","  \n","    def increment_classes(self, n=10):\n","        in_features = self.net.fc.in_features  # size of each input sample\n","        out_features = self.net.fc.out_features  # size of each output sample\n","        weight = self.net.fc.weight.data\n","        bias = self.net.fc.bias.data\n","\n","        self.net.fc = nn.Linear(in_features, out_features+n)\n","        self.net.fc.weight.data[:out_features] = weight\n","        self.net.fc.bias.data[:out_features] = bias\n","    \n","    def output_neurons_count(self):\n","        return self.net.fc.out_features\n","    \n","  \n","    \n","    def to_onehot(self, targets):\n","      \n","        num_classes = self.net.fc.out_features\n","        one_hot_targets = torch.eye(num_classes)[targets]\n","\n","        return one_hot_targets.to(self.device)\n","\n"],"execution_count":null,"outputs":[]}]}