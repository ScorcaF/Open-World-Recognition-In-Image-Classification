{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"baselines_main.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"akp--Dj9RBTj"},"source":["#import libraries and package\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"wkBUBd8bQ6HE"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n","\n","from torchvision import transforms\n","from torchvision.models import resnet34\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","from PIL import Image\n","from copy import deepcopy\n","\n","import numpy as np\n","import sys\n","import os\n","np.set_printoptions(threshold=sys.maxsize) #needed to print correctly the logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_OSaUaoksjR"},"source":["righe di codice utili per importare da altri jupyter notebook delle funzioni. "]},{"cell_type":"code","metadata":{"id":"i2MXsnVKxbzx"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrWrAj7bjtv5"},"source":["%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto\"\n","!pip install import-ipynb\n","\n","import import_ipynb "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKzmzOrAkGiM"},"source":["#retrieve from folder 'data'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/data\"                          \n","from DataClean import Cifar100 as datasetManager   # from **nome jupyter notebook** import **classe o metodo da importare**\n","\n","#retrieve from folder 'models'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/models\"\n","from resnet import resnet32\n","from Manager import Manager\n","from LwFMC import LWF\n","from iCaRL import iCaRL_rand\n","from iCaRL import Exemplars\n","from iCaRL import iCaRL_herd\n","\n","\n","#retrieve from folder 'logs'\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto/logs\"\n","from Save_logs import save_logs\n","\n","# return in the main project folder\n","%cd \"/content/drive/MyDrive/MLDL2021_Calderaro_Giannuzzi_Scorca/File colab/File progetto\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zVIwT_NfRboW"},"source":["# Arguments"]},{"cell_type":"code","metadata":{"id":"vl63oUufRbJN"},"source":["DEVICE=torch.device(\"cuda:0\")\n","DIR='/content/data'\n","\n","RANDOM_SEED = [1993,30,423]    \n","NUM_CLASSES = 100       # Total number of classes\n","\n","# Training\n","BATCH_SIZE = 64         # Batch size \n","NUM_EPOCHS = 70         # Total number of training epochs\n","LR = 2                  # Initial learning rate\n","MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n","WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n","MILESTONES = [49, 63]   # Step down policy from iCaRL (MultiStepLR)\n","                        # Decrease the learning rate by gamma at each milestone\n","GAMMA = 0.2             # Gamma factor from iCaRL\n","\n","\n","### define dataset transformation ###\n","train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),  #useful data augmentation\n","                                      transforms.RandomHorizontalFlip(),     #useful data augmentation\n","                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n","                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","test_transform = transforms.Compose([transforms.ToTensor(),\n","                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x9yXRdQ3R_Nh"},"source":["# Fine tuning implementation\n","we need to train on 10 new classes and then test on all the seen classes. \n"," \n"]},{"cell_type":"code","metadata":{"id":"idw3MOpiTm6S"},"source":["logs = [[] for _ in range(len(RANDOM_SEED))]\n","RUN_NAME = 'fine_tuning'\n","\n","for run_i in range(len(RANDOM_SEED)):\n","  #set the random seed\n","  random_seed=RANDOM_SEED[run_i]\n","\n","  #instantiate the net and loss:\n","  net=resnet32()\n","  criterion=nn.BCEWithLogitsLoss()\n"," \n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=random_seed, transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=random_seed, transform=test_transform)\n","\n","\n","  for split_i in range(10):\n","    print(f\"-- Split {split_i} of run {run_i} --\")\n","    \n","      \n","      #---- net parameters part ----\n","    parameters_to_optimize = net.parameters()\n","    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)\n","\n","      #---- dataset preparation part ----\n","    \n","    #we create the splitted version of train and test:\n","    ## 10 classes in the train (from current split), 10*(numberOfSeenSplit) for test\n","    train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","    test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","\n","    #instantiate the dataloaders:\n","    #train and validation split \n","    train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=random_seed,\n","                                          test_size = 0.1)   #choose here the test size\n","\n","    train_data_split = Subset(train_dataset , train_idx)                        #we use the indices previously extracted to make subset of \n","    val_data_split = Subset(train_dataset , val_idx)                            #the original train_dataset\n","\n","    \n","    train_dataloader=DataLoader(train_data_split, batch_size=BATCH_SIZE,        #we instantiate the dataloaders\n","                                shuffle=True, num_workers=4, drop_last=True)\n","    val_dataloader=DataLoader(val_data_split, batch_size=BATCH_SIZE, \n","                              shuffle=True, num_workers=4, drop_last=True)\n","\n","    \n","    #test dataloader with all seen classes until current iteration:\n","    test_dataloader=DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","\n","      #---- end of data preparation ----\n","\n","      #---- model training ----\n","    manager = Manager(DEVICE, net, criterion, optimizer, scheduler,\n","                          train_dataloader,\n","                          val_dataloader,\n","                          test_dataloader)\n","    \n","    manager.train(NUM_EPOCHS)  # train the model\n","    \n","    # Test the model on classes seen until now\n","    test_accuracy, all_targets, all_preds = manager.test()\n","\n","    #save test results\n","    logs[run_i].append({})\n","\n","    logs[run_i][split_i]['test_accuracy'] = test_accuracy\n","    logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","    # Add 10 nodes to last FC layer\n","    manager.increment_classes(n=10)\n","\n","\n","save_logs(logs,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfqPlud1dsI9"},"source":[" #save test results\n","logs[run_i].append({})\n","logs[run_i][split_i]['test_accuracy'] = test_accuracy\n","logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMS0s5_-_-x_"},"source":["#Learning Without Forgetting"]},{"cell_type":"code","metadata":{"id":"2evmJIS6ACqt"},"source":["RUN_NAME = 'lwf'\n","CUDA_LAUNCH_BLOCKING=1\n","logs = [[] for _ in range(len(RANDOM_SEED))]\n","\n","for run_i in range(len(RANDOM_SEED)):\n","  #set the random seed\n","  random_seed=RANDOM_SEED[run_i]\n","\n","  #instantiate the net and loss:\n","  net=resnet32()\n","  criterion= nn.BCEWithLogitsLoss()\n","\n","\n","\n","  #load the Cifar100. For each run 'random_seed' makes dataset splits different \n","  if run_i== 0: \n","    first_run = True         #we download dataset from internet into directory only at the very first instantiation\n","  else: first_run = False    \n","\n","  train_dataset = datasetManager(DIR, train=True, download=first_run, random_state=random_seed, transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=random_seed, transform=test_transform)\n","\n","\n","  for split_i in range(10):\n","      \n","    #---- net parameters part ----\n","    parameters_to_optimize = net.parameters()\n","    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)\n","\n","    #---- dataset preparation part ----\n","    \n","    #we create the splitted version of train and test:\n","    ## 10 classes in the train (from current split), 10*(numberOfSeenSplit) for test\n","    train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","    test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","\n","    #instantiate the dataloaders:\n","    #train and validation split \n","    train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=random_seed,\n","                                          test_size = 0.1)   #choose here the test size\n","\n","    train_data_split = Subset(train_dataset , train_idx)                        #we use the indices previously extracted to make subset of \n","    val_data_split = Subset(train_dataset , val_idx)                            #the original train_dataset\n","\n","    \n","    train_dataloader=DataLoader(train_data_split, batch_size=BATCH_SIZE,        #we instantiate the dataloaders\n","                                shuffle=True, num_workers=4, drop_last=True)\n","    val_dataloader=DataLoader(val_data_split, batch_size=BATCH_SIZE, \n","                              shuffle=True, num_workers=4, drop_last=True)\n","\n","    \n","    #test dataloader with all seen classes until current iteration:\n","    test_dataloader=DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n","\n","\n","    #---- end of data preparation ----\n","\n","    #---- model training ----\n","    num_classes = 10*(split_i+1)\n","\n","    if num_classes == 10: # old network == None , we are on first split\n","                lwf = LWF(DEVICE, net, None, criterion, optimizer, scheduler,\n","                            train_dataloader,\n","                            val_dataloader,\n","                            test_dataloader,\n","                            num_classes)\n","    else:\n","                lwf = LWF(DEVICE, net, old_net, criterion, optimizer, scheduler,\n","                            train_dataloader,\n","                            val_dataloader,\n","                            test_dataloader,\n","                            num_classes)\n","\n","\n","    \n","    lwf.train(NUM_EPOCHS)  # train the model\n","   \n","    \n","    # score record part\n","    ##saving score for each run and for each split_i\n","    logs[run_i].append({})\n","    \n","    # Test the model on classes seen until now\n","    test_accuracy, all_targets, all_preds = lwf.test()\n","\n","    logs[run_i][split_i]['test_accuracy'] = test_accuracy\n","    logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","    \n","    old_net = deepcopy(lwf.net)\n","\n","    lwf.increment_classes()    \n","\n","\n","save_logs(logs,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUFQ3O9_Am_h"},"source":["\n","# iCaRL (with random exemplar set construction)\n","iCaRL with random exemplar set construction. Used as comparision to herding strategy."]},{"cell_type":"code","metadata":{"id":"yFRQe5UOAqRS"},"source":["RUN_NAME = 'iCarl_no_herding_3run'\n","\n","CUDA_LAUNCH_BLOCKING=1\n","NUM_RUNS = len(RANDOM_SEED)\n","\n","\n","# Initialize logs\n","logs_icarl = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","  \n","  net = resnet32()\n","  icarl = iCaRL_rand(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n","  \n","  for split_i in range(10):\n","        print(f\"## Split {split_i} of run {run_i} ##\")\n","\n","\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),\n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   \n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)                 #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)                     #the original train_dataset\n","\n","    \n","        #N.B.: Dataloader are instantiated inside iCaRL model\n","        icarl.incremental_train(split_i, train_data_split, val_data_split)\n","\n","        #create logs for the current run\n","        logs_icarl[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","\n","        logs_icarl[run_i][split_i]['accuracy'] = acc\n","        logs_icarl[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","save_logs(logs_icarl,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-3n1JJWFDvN"},"source":["#iCaRL\n","official iCaRL as in the paper, with herding as construction strategy for exemplar set."]},{"cell_type":"code","metadata":{"id":"iZjYbhaGFFSs"},"source":["RUN_NAME = 'iCaRL_herding'\n","CUDA_LAUNCH_BLOCKING=1\n","NUM_RUNS = len(RANDOM_SEED)\n","\n","# Initialize logs\n","logs_icarl_herd = [[] for _ in range(NUM_RUNS)]\n","\n","for run_i in range(NUM_RUNS):\n","\n","  train_dataset = datasetManager(DIR, train=True, download=True, random_state=RANDOM_SEED[run_i], transform=train_transform)\n","  test_dataset = datasetManager(DIR, train=False, download=False, random_state=RANDOM_SEED[run_i], transform=test_transform)\n","\n","    \n","  net = resnet32()\n","  icarl = iCaRL_herd(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n","\n","  for split_i in range(10):\n","        print(f\"## Split {split_i} of run {run_i} ##\")\n","\n","\n","        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n","        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n","        \n","\n","        train_idx, val_idx = train_test_split(list(range(len(train_dataset))),      #we extract a split of indices\n","                                          random_state=RANDOM_SEED[run_i],\n","                                          test_size = 0.1)   \n","                                          \n","        train_data_split = Subset(train_dataset , train_idx)             #we use the indices previously extracted to make subset of \n","        val_data_split = Subset(train_dataset , val_idx)                 #the original train_dataset\n","\n","        #N.B.: Dataloader are instantiated inside iCaRL model\n","        icarl.incremental_train(split_i, train_data_split, val_data_split)\n","\n","        logs_icarl_herd[run_i].append({})\n","\n","        acc, all_targets, all_preds = icarl.test(test_dataset, train_data_split)\n","\n","        logs_icarl_herd[run_i][split_i]['accuracy'] = acc\n","        logs_icarl_herd[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n","\n","\n","save_logs(logs_icarl_herd,RANDOM_SEED, NUM_EPOCHS, RUN_NAME, BATCH_SIZE)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bYKqcC-BJvkO"},"source":[""],"execution_count":null,"outputs":[]}]}